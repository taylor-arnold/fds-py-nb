[
  {
    "objectID": "notebook20b.html",
    "href": "notebook20b.html",
    "title": "Notebook20b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook20b"
    ]
  },
  {
    "objectID": "notebook19b.html",
    "href": "notebook19b.html",
    "title": "Notebook19b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook19b"
    ]
  },
  {
    "objectID": "notebook18b.html",
    "href": "notebook18b.html",
    "title": "Notebook18b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook18b"
    ]
  },
  {
    "objectID": "notebook17b.html",
    "href": "notebook17b.html",
    "title": "Notebook17b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook17b"
    ]
  },
  {
    "objectID": "notebook16b.html",
    "href": "notebook16b.html",
    "title": "Notebook16b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook16b"
    ]
  },
  {
    "objectID": "notebook15b.html",
    "href": "notebook15b.html",
    "title": "Notebook15b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook15b"
    ]
  },
  {
    "objectID": "notebook14b.html",
    "href": "notebook14b.html",
    "title": "Notebook14b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook14b"
    ]
  },
  {
    "objectID": "notebook13b.html",
    "href": "notebook13b.html",
    "title": "Notebook13b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook13b"
    ]
  },
  {
    "objectID": "notebook12b.html",
    "href": "notebook12b.html",
    "title": "Notebook12b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook12b"
    ]
  },
  {
    "objectID": "notebook11b.html",
    "href": "notebook11b.html",
    "title": "Notebook11b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook11b"
    ]
  },
  {
    "objectID": "notebook10b.html",
    "href": "notebook10b.html",
    "title": "Notebook10b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook10b"
    ]
  },
  {
    "objectID": "notebook09b.html",
    "href": "notebook09b.html",
    "title": "Notebook09b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook09b"
    ]
  },
  {
    "objectID": "notebook08b.html",
    "href": "notebook08b.html",
    "title": "Notebook08b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook08b"
    ]
  },
  {
    "objectID": "notebook07b.html",
    "href": "notebook07b.html",
    "title": "Notebook07b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook07b"
    ]
  },
  {
    "objectID": "notebook06b.html",
    "href": "notebook06b.html",
    "title": "Notebook06b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook06b"
    ]
  },
  {
    "objectID": "notebook05b.html",
    "href": "notebook05b.html",
    "title": "Notebook05b",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook05b"
    ]
  },
  {
    "objectID": "notebook04c.html",
    "href": "notebook04c.html",
    "title": "Notebook04c",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nrva = pl.read_csv(ub + \"data/flightsrva_flights.csv.gz\", null_values=[\"NA\"])\nairport = pl.read_csv(ub + \"data/flightsrva_airports.csv.gz\", null_values=[\"NA\"])\nairline = pl.read_csv(ub + \"data/flightsrva_airlines.csv.gz\", null_values=[\"NA\"])\n\n\n\nQuestions\nIn this notebook, we will analyze flight data from Richmond International Airport (RVA). The rva dataset contains information about individual flights including departure time, destination, delays, distance, and air time. We also have lookup tables: airport contains geographic information about airports, and airline maps carrier codes to airline names.\nCount the number of flights departing at each hour of the day and create a line plot showing this pattern.\n\n(\n    rva\n    .group_by(c.hour)\n    .agg(\n        count = pl.len()\n    )\n    .pipe(ggplot, aes(\"hour\", \"count\"))\n    + geom_line()\n    + scale_x_continuous(breaks=breaks_width(1))\n)\n\n\n\n\n\n\n\n\nModify the previous plot to color the line using the hex color #F54927.\n\n(\n    rva\n    .group_by(c.hour)\n    .agg(\n        count = pl.len()\n    )\n    .pipe(ggplot, aes(\"hour\", \"count\"))\n    + geom_line(color=\"#F54927\")\n    + scale_x_continuous(breaks=breaks_width(1))\n)\n\n\n\n\n\n\n\n\nNow create a version that shows a separate line for each month, with each month in a different color.\n\n(\n    rva\n    .group_by(c.month, c.hour)\n    .agg(\n        count = pl.len()\n    )\n    .pipe(ggplot, aes(\"hour\", \"count\"))\n    + geom_line(aes(color=\"factor(month)\"))\n    + scale_x_continuous(breaks=breaks_width(1))\n    + scale_color_cmap_d()\n)\n\n\n\n\n\n\n\n\nFor each destination, compute the average air time and average distance. Create a scatter plot of these averages and add a linear trend line.\n\n(\n    rva\n    .group_by(c.dest)\n    .agg(\n        air_time_mean = c.air_time.mean(),\n        distance_mean = c.distance.mean(),\n        count = pl.len()\n    )\n    .pipe(ggplot, aes(\"air_time_mean\", \"distance_mean\"))\n    + geom_point()\n    + geom_smooth(method=\"lm\", se=False)\n)\n\n\n\n\n\n\n\n\nFor each destination, compute the proportion of flights that arrive more than 15 minutes late and the proportion that depart more than 15 minutes late. Filter to destinations with more than 1000 flights. Create a scatter plot of these delay rates with destination labels.\n\n(\n    rva\n    .group_by(c.dest)\n    .agg(\n        arr_delay_avg = (c.arr_delay &gt; 15).mean(),\n        dep_delay_avg = (c.dep_delay &gt; 15).mean(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 1000)\n    .pipe(ggplot, aes(\"arr_delay_avg\", \"dep_delay_avg\"))\n    + geom_point()\n    + geom_text(aes(label=\"dest\"), nudge_y=0.003)\n)\n\n\n\n\n\n\n\n\nFor each hour of the day, compute the proportion of flights that arrive more than 30 minutes late. Create a scatter plot showing this pattern.\n\n(\n    rva\n    .group_by(c.hour)\n    .agg(\n        delayed_prop = (c.arr_delay &gt; 30).mean()\n    )\n    .pipe(ggplot, aes(\"factor(hour)\", \"delayed_prop\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nCount the number of flights for each carrier and join this to the airline table to get the full airline names. Create a horizontal bar chart of flight counts, ordered from fewest to most flights.\n\n(\n    rva\n    .group_by(c.carrier)\n    .agg(count = pl.len())\n    .join(airline, on=\"carrier\")\n    .pipe(ggplot, aes(\"reorder(name, count)\", \"count\"))\n    + geom_col()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nCount the number of flights to each destination and join this to the airport table to get the latitude and longitude of each destination. Create a scatter plot using longitude and latitude as the axes, with point sizes proportional to the number of flights. This creates a simple map of where flights from Richmond go.\n\n(\n    rva\n    .group_by(c.dest)\n    .agg(count = pl.len())\n    .join(airport, left_on=\"dest\", right_on=\"faa\")\n    .pipe(ggplot, aes(\"lon\", \"lat\"))\n    + geom_point(aes(size=\"count\"))\n    + scale_size_area()\n)",
    "crumbs": [
      "Notebook04c"
    ]
  },
  {
    "objectID": "notebook04a.html",
    "href": "notebook04a.html",
    "title": "Notebook04a",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nmovie = pl.read_csv(ub + \"data/movies_50_years.csv\")\ncolor = pl.read_csv(ub + \"data/movies_50_years_color.csv\")\ngenre = pl.read_csv(ub + \"data/movies_50_years_genre.csv\")\npeople = pl.read_csv(ub + \"data/movies_50_years_people.csv\")\n\n\n\nQuestions\nIn this notebook, we will learn how to combine information from multiple datasets using joins. The movie data has been split into several tables: movie contains basic information about each film (runtime, rating, etc.), genre lists the genres associated with each film, color contains information about the colors in each movie’s poster, and people has information about the cast and crew. Because some movie titles are duplicated across different years, we will always join by both year and title to ensure we match records correctly.\nCreate a new column in the movie dataset that computes the ratio of the number of ratings to the runtime. This gives a rough sense of audience engagement per minute of film.\n\n(\n    movie\n    .with_columns(\n        ratings_per_minute = c.rating_count / c.runtime\n    )\n)\n\n\nshape: (5_000, 13)\n\n\n\nyear\ntitle\nmpa\nruntime\ngross\nrating_count\nrating\nmetacritic\nposter_brightness\nposter_saturation\nposter_edgeness\ndescription\nratings_per_minute\n\n\ni64\nstr\nstr\ni64\nf64\ni64\nf64\ni64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n1970\n\"Love Story\"\n\"PG\"\n100\n106.4\n28330\n6.9\nnull\n79.039734\n8.029792\n4.586166\n\"A boy and a girl from differen…\n283.3\n\n\n1970\n\"Airport\"\n\"G\"\n137\n100.49\n16512\n6.6\n42\n70.73516\n29.284572\n4.954735\n\"A bomber on board an airplane,…\n120.525547\n\n\n1970\n\"MASH\"\n\"R\"\n116\n81.6\n64989\n7.5\nnull\n74.540002\n40.103629\n3.510285\n\"The staff of a Korean War fiel…\n560.25\n\n\n1970\n\"Patton\"\n\"GP\"\n172\n61.7\n90461\n7.9\nnull\n83.128991\n17.43385\n3.657574\n\"The World War II phase of the …\n525.936047\n\n\n1970\n\"The AristoCats\"\n\"G\"\n78\n37.68\n87551\n7.1\nnull\n79.794746\n12.481992\n4.400358\n\"With the help of a smooth talk…\n1122.448718\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2019\n\"The Art of Self-Defense\"\n\"R\"\n104\n2.41\n18474\n6.7\nnull\n45.466534\n67.429893\n3.232832\n\"After being attacked on the st…\n177.634615\n\n\n2019\n\"Luce\"\n\"R\"\n109\n1.78\n5800\n6.8\nnull\n76.585779\n5.941685\n2.307601\n\"A married couple is forced to …\n53.211009\n\n\n2019\n\"The Other Side of Heaven 2: Fi…\n\"PG-13\"\n117\n1.72\n259\n5.1\nnull\n57.68667\n39.572749\n2.307937\n\"Missionary John H. Groberg ret…\n2.213675\n\n\n2019\n\"The Aftermath\"\n\"R\"\n108\n1.62\n13203\n6.3\n43\n18.4582\n36.364627\n2.137566\n\"Post World War II, a British c…\n122.25\n\n\n2019\n\"The Kid\"\n\"R\"\n100\n1.51\n5210\n5.9\n51\n27.409446\n40.325301\n3.192685\n\"The story of a young boy who w…\n52.1\n\n\n\n\n\n\nUsing the movie dataset, create a scatter plot with runtime on the x-axis and the IMDB rating on the y-axis.\n\n(\n    movie\n    .pipe(ggplot, aes(\"runtime\", \"rating\"))\n    + geom_point()\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:374: PlotnineWarning: geom_point : Removed 387 rows containing missing values.\n\n\n\n\n\n\n\n\n\nCreate a boxplot showing the distribution of poster brightness for each MPA rating category.\n\n(\n    movie\n    .pipe(ggplot, aes(\"mpa\", \"poster_brightness\"))\n    + geom_boxplot()\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_boxplot : Removed 461 rows containing non-finite values.\n\n\n\n\n\n\n\n\n\nNow let’s start working with multiple tables. Join the genre table to the movie table. Remember to join on both year and title since some titles appear in multiple years.\n\n(\n    genre\n    .join(movie, on=[c.year, c.title])\n)\n\n\nshape: (11_887, 13)\n\n\n\nyear\ntitle\ngenre\nmpa\nruntime\ngross\nrating_count\nrating\nmetacritic\nposter_brightness\nposter_saturation\nposter_edgeness\ndescription\n\n\ni64\nstr\nstr\nstr\ni64\nf64\ni64\nf64\ni64\nf64\nf64\nf64\nstr\n\n\n\n\n1970\n\"Love Story\"\n\"Drama\"\n\"PG\"\n100\n106.4\n28330\n6.9\nnull\n79.039734\n8.029792\n4.586166\n\"A boy and a girl from differen…\n\n\n1970\n\"Love Story\"\n\"Romance\"\n\"PG\"\n100\n106.4\n28330\n6.9\nnull\n79.039734\n8.029792\n4.586166\n\"A boy and a girl from differen…\n\n\n1970\n\"Airport\"\n\"Action\"\n\"G\"\n137\n100.49\n16512\n6.6\n42\n70.73516\n29.284572\n4.954735\n\"A bomber on board an airplane,…\n\n\n1970\n\"Airport\"\n\"Drama\"\n\"G\"\n137\n100.49\n16512\n6.6\n42\n70.73516\n29.284572\n4.954735\n\"A bomber on board an airplane,…\n\n\n1970\n\"Airport\"\n\"Thriller\"\n\"G\"\n137\n100.49\n16512\n6.6\n42\n70.73516\n29.284572\n4.954735\n\"A bomber on board an airplane,…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2019\n\"The Aftermath\"\n\"Romance\"\n\"R\"\n108\n1.62\n13203\n6.3\n43\n18.4582\n36.364627\n2.137566\n\"Post World War II, a British c…\n\n\n2019\n\"The Aftermath\"\n\"War\"\n\"R\"\n108\n1.62\n13203\n6.3\n43\n18.4582\n36.364627\n2.137566\n\"Post World War II, a British c…\n\n\n2019\n\"The Kid\"\n\"Biography\"\n\"R\"\n100\n1.51\n5210\n5.9\n51\n27.409446\n40.325301\n3.192685\n\"The story of a young boy who w…\n\n\n2019\n\"The Kid\"\n\"Drama\"\n\"R\"\n100\n1.51\n5210\n5.9\n51\n27.409446\n40.325301\n3.192685\n\"The story of a young boy who w…\n\n\n2019\n\"The Kid\"\n\"Western\"\n\"R\"\n100\n1.51\n5210\n5.9\n51\n27.409446\n40.325301\n3.192685\n\"The story of a young boy who w…\n\n\n\n\n\n\nUsing the joined data, compute the average poster brightness and average poster saturation for each genre. Then create a scatter plot of these averages with genre labels next to each point.\n\n(\n    genre\n    .join(movie, on=[c.year, c.title])\n    .group_by(c.genre)\n    .agg(\n        poster_brightness_mean = c.poster_brightness.mean(),\n        poster_saturation = c.poster_saturation.mean()\n    )\n    .pipe(ggplot, aes(\"poster_brightness_mean\", \"poster_saturation\"))\n    + geom_point()\n    + geom_text(aes(label=\"genre\"), nudge_y=2)\n)\n\n\n\n\n\n\n\n\nThe people table contains information about cast and crew, including a predicted gender and a confidence score for that prediction. Filter the people data to only starring roles where the gender confidence is greater than 0.8. Then join this to the genre table. For each genre, compute the proportion of these roles that are female and the total count. Sort by the proportion of female roles and display all results.\n\n(\n    people\n    .filter(c.role == \"starring\")\n    .filter(c.gender_conf &gt; 0.8)\n    .join(genre, on=[c.year, c.title])\n    .group_by(c.genre)\n    .agg(\n        avg_female = (c.gender == \"female\").mean(),\n        count = pl.count()\n    )\n    .sort(c.avg_female)\n    .pipe(print_rows)\n)\n\n/var/folders/5w/rxshd8hd6vv8mxsbj211mr_m0000gn/T/ipykernel_48472/3827170702.py:9: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n\n\n\nshape: (21, 3)\n\n\n\ngenre\navg_female\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"Short\"\n0.0\n1\n\n\n\"War\"\n0.183099\n213\n\n\n\"History\"\n0.233516\n364\n\n\n\"Western\"\n0.247059\n255\n\n\n\"Action\"\n0.267078\n4538\n\n\n\"Crime\"\n0.284756\n3129\n\n\n\"Adventure\"\n0.289352\n3691\n\n\n\"Sport\"\n0.293249\n474\n\n\n\"Biography\"\n0.297017\n771\n\n\n\"Animation\"\n0.299799\n994\n\n\n\"Sci-Fi\"\n0.308668\n1419\n\n\n\"Thriller\"\n0.331402\n2589\n\n\n\"Comedy\"\n0.356179\n7485\n\n\n\"Fantasy\"\n0.35889\n1513\n\n\n\"Drama\"\n0.359471\n7945\n\n\n\"Music\"\n0.364\n500\n\n\n\"Family\"\n0.364198\n1458\n\n\n\"Mystery\"\n0.37883\n1436\n\n\n\"Musical\"\n0.378995\n219\n\n\n\"Horror\"\n0.385405\n1754\n\n\n\"Romance\"\n0.433153\n2775\n\n\n\n\n\n\nLet’s find the most common dominant poster color for each genre. Start with the color table and filter to just the hue colors. For each movie, find the color with the highest percentage (this is the dominant hue). Keep only movies where the dominant hue makes up more than 5% of the poster. Join this to the genre table, then count how often each color appears as the dominant hue within each genre. Finally, find the single most common dominant color for each genre.\n\n(\n    color\n    .filter(c.color_type == \"hue\")\n    .sort(c.percentage, descending=True)\n    .group_by(c.year, c.title)\n    .head(1)\n    .filter(c.percentage &gt; 5)\n    .join(genre, on=[c.year, c.title])\n    .group_by(c.genre, c.color)\n    .agg(count = pl.count())\n    .sort(c.count, descending=True)\n    .group_by(c.genre)\n    .head(1)\n    .pipe(print_rows)\n)\n\n/var/folders/5w/rxshd8hd6vv8mxsbj211mr_m0000gn/T/ipykernel_48472/1500220528.py:10: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n\n\n\nshape: (21, 3)\n\n\n\ngenre\ncolor\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"Music\"\n\"red\"\n70\n\n\n\"Western\"\n\"red\"\n30\n\n\n\"Fantasy\"\n\"red\"\n148\n\n\n\"Biography\"\n\"red\"\n84\n\n\n\"Animation\"\n\"blue\"\n132\n\n\n\"History\"\n\"red\"\n39\n\n\n\"War\"\n\"red\"\n28\n\n\n\"Comedy\"\n\"red\"\n1018\n\n\n\"Musical\"\n\"red\"\n30\n\n\n\"Crime\"\n\"red\"\n432\n\n\n\"Short\"\n\"red\"\n1\n\n\n\"Horror\"\n\"red\"\n160\n\n\n\"Family\"\n\"red\"\n159\n\n\n\"Action\"\n\"red\"\n535\n\n\n\"Adventure\"\n\"blue\"\n374\n\n\n\"Sport\"\n\"red\"\n69\n\n\n\"Drama\"\n\"red\"\n971\n\n\n\"Romance\"\n\"red\"\n436\n\n\n\"Sci-Fi\"\n\"blue\"\n155\n\n\n\"Thriller\"\n\"red\"\n294\n\n\n\"Mystery\"\n\"red\"\n139\n\n\n\n\n\n\nNow, let’s see an example of how to use an semi-join. First, create a table called movie_long that consists of all films that have a run time greater than 2.5 hours (150 minutes).\n\nmovie_long = movie.filter(c.runtime &gt; 2.5 * 60)\n\nThen, draw a plot showing the number films that have a long runtime by their genre using a semi-join. Try to make the plot look nice. Note: Yes, you could do this in other ways in this simple case. Semi- and anti-joins are very useful as our analyses become more complex.\n\n(\n    genre\n    .join(movie_long, on=[c.year, c.title], how=\"semi\")\n    .group_by(c.genre)\n    .agg(count = pl.len())\n    .pipe(ggplot, aes(\"reorder(genre, count)\", \"count\"))\n    + geom_col()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nWe’ve mentioned that some titles are repeated in the data and therefore we always need to group by title and year. Find all of the titles that are repeated. Print all of the rows of the result and order by the counts.\n\n(\n    movie\n    .group_by(c.title)\n    .agg(count = pl.len())\n    .filter(c.count &gt; 1)\n    .sort(c.count, descending=True)\n    .pipe(print_rows)\n)\n\n\nshape: (92, 2)\n\n\n\ntitle\ncount\n\n\nstr\nu32\n\n\n\n\n\"The Adventures of Pinocchio\"\n3\n\n\n\"Shaft\"\n3\n\n\n\"Robin Hood\"\n3\n\n\n\"Hercules\"\n3\n\n\n\"Halloween\"\n3\n\n\n\"Godzilla\"\n3\n\n\n\"Hairspray\"\n2\n\n\n\"Twilight\"\n2\n\n\n\"Threesome\"\n2\n\n\n\"Sparkle\"\n2\n\n\n\"Aladdin\"\n2\n\n\n\"Scream\"\n2\n\n\n\"Child's Play\"\n2\n\n\n\"Hot Pursuit\"\n2\n\n\n\"Annie\"\n2\n\n\n\"Freaky Friday\"\n2\n\n\n\"Overboard\"\n2\n\n\n\"Grease 2\"\n2\n\n\n\"The Circle\"\n2\n\n\n\"Clash of the Titans\"\n2\n\n\n\"The Karate Kid\"\n2\n\n\n\"The Hills Have Eyes\"\n2\n\n\n\"The Lion King\"\n2\n\n\n\"The Getaway\"\n2\n\n\n\"The Amityville Horror\"\n2\n\n\n\"Bad Boys\"\n2\n\n\n\"Death Wish\"\n2\n\n\n\"The Three Musketeers\"\n2\n\n\n\"The Stepford Wives\"\n2\n\n\n\"Charlotte's Web\"\n2\n\n\n\"When a Stranger Calls\"\n2\n\n\n\"A Star Is Born\"\n2\n\n\n\"Serenity\"\n2\n\n\n\"The Strangers\"\n2\n\n\n\"The Guardian\"\n2\n\n\n\"Halloween II\"\n2\n\n\n\"The Jungle Book\"\n2\n\n\n\"Poltergeist\"\n2\n\n\n\"Going in Style\"\n2\n\n\n\"Friday the 13th\"\n2\n\n\n\"Total Recall\"\n2\n\n\n\"Gone in 60 Seconds\"\n2\n\n\n\"RoboCop\"\n2\n\n\n\"Grease\"\n2\n\n\n\"The Rookie\"\n2\n\n\n\"Life\"\n2\n\n\n\"The Intruder\"\n2\n\n\n\"Point Break\"\n2\n\n\n\"Carrie\"\n2\n\n\n\"No Escape\"\n2\n\n\n\"Pete's Dragon\"\n2\n\n\n\"Neighbors\"\n2\n\n\n\"Night School\"\n2\n\n\n\"The Avengers\"\n2\n\n\n\"Mother\"\n2\n\n\n\"Footloose\"\n2\n\n\n\"Teenage Mutant Ninja Turtles\"\n2\n\n\n\"The Crazies\"\n2\n\n\n\"Crossroads\"\n2\n\n\n\"Les Misérables\"\n2\n\n\n\"The Green Hornet\"\n2\n\n\n\"Fantastic Four\"\n2\n\n\n\"A Nightmare on Elm Street\"\n2\n\n\n\"Beauty and the Beast\"\n2\n\n\n\"The Kid\"\n2\n\n\n\"Red Dawn\"\n2\n\n\n\"Nobody's Fool\"\n2\n\n\n\"The Last House on the Left\"\n2\n\n\n\"Mother Knows Best\"\n2\n\n\n\"The Addams Family\"\n2\n\n\n\"Dawn of the Dead\"\n2\n\n\n\"Pet Sematary\"\n2\n\n\n\"The Heartbreak Kid\"\n2\n\n\n\"Emma\"\n2\n\n\n\"The Animal\"\n2\n\n\n\"The Omen\"\n2\n\n\n\"My Little Pony: The Movie\"\n2\n\n\n\"A Christmas Carol\"\n2\n\n\n\"The Island\"\n2\n\n\n\"Ghostbusters\"\n2\n\n\n\"Project X\"\n2\n\n\n\"The Fog\"\n2\n\n\n\"Vacation\"\n2\n\n\n\"Gladiator\"\n2\n\n\n\"The Island of Dr. Moreau\"\n2\n\n\n\"Hellboy\"\n2\n\n\n\"Sahara\"\n2\n\n\n\"King Kong\"\n2\n\n\n\"The Player\"\n2\n\n\n\"Billy Boy\"\n2\n\n\n\"The Mummy\"\n2\n\n\n\"Arthur\"\n2",
    "crumbs": [
      "Notebook04a"
    ]
  },
  {
    "objectID": "notebook03d.html",
    "href": "notebook03d.html",
    "title": "Notebook03d",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\")\n\n\n\nQuestions\nIn this notebook, we will continue exploring data visualization using the food dataset. We’ll start with a few review questions before moving on to some new techniques for layering data, adding labels, creating faceted plots, and customizing plot annotations.\nCreate a histogram of the calorie content of the foods in the dataset. Use a bin width of 25 calories.\n\n(\n    food\n    .pipe(ggplot, aes(\"calories\"))\n    + geom_histogram(fill=\"white\", color=\"black\", binwidth=25, boundary=0)\n)\n\n\n\n\n\n\n\n\nCreate a boxplot showing the distribution of protein content for each food group. Order the categories in the plot by their protein amount.\n\n(\n    food\n    .pipe(ggplot, aes(\"reorder(food_group, protein)\", \"protein\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nWe will look again at the smaller dataset of food items today to get a sense of how the final set of graphics commands works. Start by picking a fun color from the Color Wheel. We will use this throughout the notebook.\nStart by creating a dataset containing only dairy products. Then, plot the full food dataset with total fat on the x-axis and calories on the y-axis. Color these points in light grey (#d3d3d3). Layer the dairy products on top using the accent color that you selected.\n\nfood_dairy = food.filter(c.food_group == \"dairy\")\n(\n    food\n    .pipe(ggplot, aes(\"total_fat\", \"calories\"))\n    + geom_point(color=\"#d3d3d3\")\n    + geom_point(color=\"#8916D0\", data=food_dairy)\n)\n\n\n\n\n\n\n\n\nNow add the names of the dairy products to the plot. Use your accent color for the text and add a vertical nudge so the labels don’t overlap with the points.\n\n(\n    food\n    .pipe(ggplot, aes(\"total_fat\", \"calories\"))\n    + geom_point(color=\"#d3d3d3\")\n    + geom_point(color=\"#8916D0\", data=food_dairy)\n    + geom_text(aes(label=\"item\"), color=\"#8916D0\", data=food_dairy, nudge_y=10)\n)\n\n\n\n\n\n\n\n\nCreate a scatter plot of total fat versus calories, with the points colored by food group. Use faceting to create a separate panel for each food group. Since the color and facet show the same information, you can hide the legend.\n\n(\n    food\n    .pipe(ggplot, aes(\"total_fat\", \"calories\"))\n    + geom_point(aes(color=\"food_group\"), show_legend=False)\n    + facet_wrap(\"food_group\")\n    + scale_color_cmap_d()\n)\n\n\n\n\n\n\n\n\nCreate a scatter plot of total fat versus calories, colored by food group. This time, add informative manually created labels: a title, subtitle, caption, and clear axis and legend labels.\n\n(\n    food\n    .pipe(ggplot, aes(\"total_fat\", \"calories\"))\n    + geom_point(aes(color=\"food_group\"))\n    + scale_color_cmap_d()\n    + labs(\n        x=\"Total grams of fat in 100g serving\",\n        y=\"Total calories in 100g serving\",\n        color=\"Food group\",\n        title=\"Relationship between fat and calories\",\n        subtitle=\"A study of 61 common food types\",\n        caption=\"Data from WikiData\"\n    )\n)",
    "crumbs": [
      "Notebook03d"
    ]
  },
  {
    "objectID": "notebook03b.html",
    "href": "notebook03b.html",
    "title": "Notebook03a",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nmetro = pl.read_csv(ub + \"data/acs_cbsa.csv\").filter(c.quad != \"O\")\n\n\n\nQuestions\nToday, we will look at the metro dataset. Start by plotting the household median income on the x-axis of the regions on the x-axis and the density on the y-axis.\n\n(\n    metro\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nThis is going to be messy. Let’s modify the plot to only include those where the population is greater than 2 million people (remember, population is recorded in millions in the data).\n\n(\n    metro\n    .filter(c.pop &gt; 2)\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nRecreate the previous plot, but this time color the points by division. Notice that color goes inside aes() when mapping to a variable.\n\n(\n    metro\n    .filter(c.pop &gt; 2)\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_point(aes(color=\"division\"))\n)\n\n\n\n\n\n\n\n\nUse a Color Wheel to select a fun color. In the code below, update the previous plot to use this fixed color in place of the divsion-based colors. Notice that when using a fixed color (not mapped to data), color goes outside of aes().\n\n# Of course, you will have a different color value in your own code,\n# but the rest should look the same.\n(\n    metro\n    .filter(c.pop &gt; 2)\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_point(color=\"#E3763B\")\n)\n\n\n\n\n\n\n\n\nNow, instead of points, use text labels to display the item names. Replace geom_point() with geom_text() and map the label aesthetic to the name column. Keep the fixed color you choose.\n\n(\n    metro\n    .filter(c.pop &gt; 2)\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_text(aes(label=\"name\"), color=\"#E3763B\")\n)\n\n\n\n\n\n\n\n\nNext, add both points and text labels to the plot. Use nudge_y to shift the text up slightly so it doesn’t overlap with the points and set the size a bit smaller to make it all easier to read. Keep the fixed color, keeping in mind where it needs to go in the code.\n\n# The specific values for size and nudge_y are flexible; as long as\n# it looks good your fine!\n(\n    metro\n    .filter(c.pop &gt; 2)\n    .pipe(ggplot, aes(\"hh_income_median\", \"density\"))\n    + geom_point(aes(label=\"name\"), color=\"#E3763B\")\n    + geom_text(aes(label=\"name\"), color=\"#E3763B\", size=8, nudge_y=25)\n)\n\n\n\n\n\n\n\n\nNow, summarize the data by counting the number of metro regions (from the whole dataset, not just those with a population greater than 2 million) within each division. Plot this using a bar/column plot with division on the x-axis and count on the y-axis.\n\n(\n    metro\n    .group_by(c.division)\n    .agg(count = pl.len())\n    .pipe(ggplot, aes(\"division\", \"count\"))\n    + geom_col()\n)\n\n\n\n\n\n\n\n\nModify your previous plot by flipping the axes and ordering the divisions by the counts.\n\n(\n    metro\n    .group_by(c.division)\n    .agg(count = pl.len())\n    .pipe(ggplot, aes(\"reorder(division, count)\", \"count\"))\n    + geom_col()\n    + coord_flip()\n)\n\n\n\n\n\n\n\n\nFinally, modify the plot by making the bars filled with the custom color you selected above.\n\n(\n    metro\n    .group_by(c.division)\n    .agg(count = pl.len())\n    .pipe(ggplot, aes(\"reorder(division, count)\", \"count\"))\n    + geom_col(fill=\"#E3763B\")\n    + coord_flip()\n)",
    "crumbs": [
      "Notebook03b"
    ]
  },
  {
    "objectID": "notebook02d.html",
    "href": "notebook02d.html",
    "title": "Notebook02d",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nmetro = pl.read_csv(ub + \"data/acs_cbsa.csv\")\n\n\n\nQuestions\nThis notebook is a review of the data manipulation techniques we have learned so far. We will work with the metro dataset, which contains information about US metropolitan areas from the American Community Survey. The dataset includes variables such as population (in millions), population density, median age, median household income, home ownership rates, median rent for a one-bedroom apartment, and geographic information including which census division each metro area belongs to.\nSort the metropolitan areas by population, with the largest metros at the top.\n\n(\n    metro\n    .sort(c.pop, descending=True)\n)\n\n\nshape: (934, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n\n\n\n\n\n\nFind the metro area with the highest median household income.\n\n(\n    metro\n    .sort(c.hh_income_median, descending=True)\n    .head(1)\n)\n\n\nshape: (1, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"San Jose\"\n41940\n\"W\"\n-121.372676\n36.908496\n1.995351\n286.41767\n41.4\n138370\n58.5\n2227\n27.8\n\"Pacific\"\n\n\n\n\n\n\nSort the data by median age, with the youngest metros first. Which places have the youngest populations?\n\n(\n    metro\n    .sort(c.age_median)\n)\n\n\nshape: (934, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"Rexburg\"\n39940\n\"W\"\n-111.518023\n44.139654\n0.064349\n10.507575\n29.5\n55621\n60.4\n743\n35.2\n\"Mountain\"\n\n\n\"Jacksonville\"\n27340\n\"S\"\n-77.432055\n34.732017\n0.201597\n97.063568\n29.6\n54732\n57.3\n646\n29.6\n\"South Atlantic\"\n\n\n\"Fort Leonard Wood\"\n22780\n\"NC\"\n-92.20759\n37.824461\n0.05346\n37.446904\n31.4\n59252\n56.2\n500\n22.8\n\"West North Central\"\n\n\n\"Laramie\"\n29660\n\"W\"\n-105.724306\n41.652174\n0.037311\n3.348232\n31.9\n50733\n54.9\n681\n35.1\n\"Mountain\"\n\n\n\"Pullman\"\n39420\n\"W\"\n-117.523085\n46.900806\n0.048197\n8.566939\n33.0\n43613\n51.4\n692\n40.9\n\"Pacific\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Rockport\"\n40530\n\"S\"\n-96.993507\n28.124829\n0.024149\n31.96932\n47.5\n51509\n79.0\n737\n27.7\n\"West South Central\"\n\n\n\"Torrington\"\n45860\n\"NE\"\n-73.245421\n41.792216\n0.185175\n75.802872\n47.8\n84797\n80.3\n948\n29.6\n\"New England\"\n\n\n\"Barnstable Town\"\n12700\n\"NE\"\n-70.290799\n41.724385\n0.227942\n200.358553\n48.5\n82619\n82.0\n926\n33.3\n\"New England\"\n\n\n\"Punta Gorda\"\n39460\n\"S\"\n-81.912254\n26.905608\n0.184837\n95.408719\n48.7\n57887\n82.3\n853\n34.8\n\"South Atlantic\"\n\n\n\"Vineyard Haven\"\n47240\n\"NE\"\n-70.650085\n41.396102\n0.020277\n66.210273\n48.8\n77392\n75.7\n1090\n30.6\n\"New England\"\n\n\n\n\n\n\nSelect only the metropolitan areas located in the South (where the quad column equals “S”).\n\n(\n    metro\n    .filter(c.quad == \"S\")\n)\n\n\nshape: (377, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n\n\n\"Washington\"\n47900\n\"S\"\n-77.513075\n38.812484\n6.332069\n363.732689\n42.4\n111252\n67.4\n1601\n28.8\n\"South Atlantic\"\n\n\n\"Miami\"\n33100\n\"S\"\n-80.506307\n26.155369\n6.105897\n430.103162\n43.9\n62870\n60.8\n1230\n36.8\n\"South Atlantic\"\n\n\n\"Atlanta\"\n12060\n\"S\"\n-84.399567\n33.691787\n6.026734\n263.275821\n41.9\n75267\n67.3\n1181\n30.3\n\"South Atlantic\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Sweetwater\"\n45020\n\"S\"\n-100.405986\n32.303445\n0.014727\n6.220819\n41.2\n44700\n62.6\n577\n24.8\n\"West South Central\"\n\n\n\"Pecos\"\n37780\n\"S\"\n-103.669712\n31.43003\n0.014667\n1.70508\n39.1\n53448\n74.7\n818\n27.4\n\"West South Central\"\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n\n\n\n\n\n\nSelect metros that have a population density greater than 500 people per square mile.\n\n(\n    metro\n    .filter(c.density &gt; 500)\n)\n\n\nshape: (11, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n\n\n\"Philadelphia\"\n37980\n\"NE\"\n-75.302635\n39.905213\n6.215222\n506.06813\n42.6\n79070\n71.1\n1083\n30.0\n\"Middle Atlantic\"\n\n\n\"Boston\"\n14460\n\"NE\"\n-71.099912\n42.555194\n4.91203\n517.827702\n42.2\n99039\n66.4\n1390\n29.5\n\"New England\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"San Juan\"\n41980\n\"O\"\n-66.112516\n18.27534\n2.096657\n554.930629\n41.8\n24755\n69.3\n379\n31.9\n\"South Atlantic\"\n\n\n\"Urban Honolulu\"\n46520\n\"O\"\n-158.092955\n21.504652\n1.015167\n631.92199\n41.3\n92600\n61.4\n1399\n33.4\n\"Pacific\"\n\n\n\"Bridgeport\"\n14860\n\"NE\"\n-73.389374\n41.270839\n0.956446\n566.341806\n44.9\n101194\n69.3\n1329\n32.2\n\"New England\"\n\n\n\"New Haven\"\n35300\n\"NE\"\n-72.931983\n41.410527\n0.864751\n538.957509\n43.3\n75043\n66.0\n1040\n30.7\n\"New England\"\n\n\n\"Trenton\"\n45940\n\"NE\"\n-74.701703\n40.283417\n0.384951\n650.172055\n44.7\n85687\n65.7\n1126\n30.2\n\"Middle Atlantic\"\n\n\n\n\n\n\nFind all metros where residents spend more than 30 percent of their income on rent.\n\n(\n    metro\n    .filter(c.rent_perc_income &gt; 30)\n)\n\n\nshape: (256, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"Miami\"\n33100\n\"S\"\n-80.506307\n26.155369\n6.105897\n430.103162\n43.9\n62870\n60.8\n1230\n36.8\n\"South Atlantic\"\n\n\n\"Atlanta\"\n12060\n\"S\"\n-84.399567\n33.691787\n6.026734\n263.275821\n41.9\n75267\n67.3\n1181\n30.3\n\"South Atlantic\"\n\n\n\"Riverside\"\n40140\n\"W\"\n-116.127488\n34.549836\n4.580402\n64.511974\n40.2\n73424\n65.2\n1120\n34.0\n\"Pacific\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Helena\"\n25760\n\"S\"\n-90.849045\n34.427273\n0.016923\n8.97778\n44.8\n32235\n50.4\n406\n33.5\n\"West South Central\"\n\n\n\"Fairfield\"\n21840\n\"NC\"\n-91.948906\n41.03177\n0.015846\n14.022545\n45.7\n47950\n66.3\n498\n32.0\n\"West North Central\"\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n\n\n\n\n\n\nSelect metros that are in either the Pacific or New England divisions.\n\n(\n    metro\n    .filter(c.division.is_in([\"Pacific\", \"New England\"]))\n)\n\n\nshape: (108, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"Boston\"\n14460\n\"NE\"\n-71.099912\n42.555194\n4.91203\n517.827702\n42.2\n99039\n66.4\n1390\n29.5\n\"New England\"\n\n\n\"San Francisco\"\n41860\n\"W\"\n-122.166182\n37.780765\n4.725584\n712.992772\n41.9\n118547\n58.4\n1940\n28.3\n\"Pacific\"\n\n\n\"Riverside\"\n40140\n\"W\"\n-116.127488\n34.549836\n4.580402\n64.511974\n40.2\n73424\n65.2\n1120\n34.0\n\"Pacific\"\n\n\n\"Seattle\"\n42660\n\"W\"\n-121.853072\n47.554512\n3.971125\n256.189989\n40.6\n97675\n64.7\n1478\n29.1\n\"Pacific\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Hood River\"\n26220\n\"W\"\n-121.650975\n45.518799\n0.023915\n17.352394\n42.4\n77815\n72.1\n670\n26.3\n\"Pacific\"\n\n\n\"Brookings\"\n15060\n\"W\"\n-124.156281\n42.456681\n0.023234\n5.491924\n47.1\n57553\n76.3\n782\n27.7\n\"Pacific\"\n\n\n\"Othello\"\n36830\n\"W\"\n-118.561139\n46.983525\n0.020353\n4.081837\n39.9\n54573\n60.5\n602\n25.2\n\"Pacific\"\n\n\n\"Vineyard Haven\"\n47240\n\"NE\"\n-70.650085\n41.396102\n0.020277\n66.210273\n48.8\n77392\n75.7\n1090\n30.6\n\"New England\"\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n\n\n\n\n\n\nFind metros in the South that have a median household income above $80,000. Use two separate filter steps.\n\n(\n    metro\n    .filter(c.quad == \"S\")\n    .filter(c.hh_income_median &gt; 80_000)\n)\n\n\nshape: (7, 13)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"Washington\"\n47900\n\"S\"\n-77.513075\n38.812484\n6.332069\n363.732689\n42.4\n111252\n67.4\n1601\n28.8\n\"South Atlantic\"\n\n\n\"Baltimore\"\n12580\n\"S\"\n-76.579833\n39.3381\n2.837237\n404.661335\n42.4\n87513\n70.6\n1132\n30.0\n\"South Atlantic\"\n\n\n\"Austin\"\n12420\n\"S\"\n-97.654303\n30.261635\n2.2343\n201.447732\n39.6\n85398\n64.0\n1216\n28.8\n\"West South Central\"\n\n\n\"Raleigh\"\n39580\n\"S\"\n-78.46101\n35.756718\n1.391801\n250.282432\n41.8\n83581\n69.6\n1104\n28.1\n\"South Atlantic\"\n\n\n\"Midland\"\n33260\n\"S\"\n-101.991236\n32.08914\n0.172177\n36.548597\n38.9\n87812\n70.7\n1128\n26.8\n\"West South Central\"\n\n\n\"California\"\n15680\n\"S\"\n-76.605579\n38.302342\n0.113209\n108.79725\n42.4\n102859\n75.6\n1266\n26.4\n\"South Atlantic\"\n\n\n\"Andrews\"\n11380\n\"S\"\n-102.637752\n32.305129\n0.018184\n4.673626\n38.5\n80518\n69.2\n1066\n25.0\n\"West South Central\"\n\n\n\n\n\n\nFor each census division, compute the average median household income and the number of metros in that division.\n\n(\n    metro\n    .group_by(c.division)\n    .agg(\n        hh_income_mean = c.hh_income_median.mean(),\n        count = pl.len()\n    )\n)\n\n\nshape: (9, 3)\n\n\n\ndivision\nhh_income_mean\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"Mountain\"\n63649.376344\n93\n\n\n\"Pacific\"\n70178.268293\n82\n\n\n\"East South Central\"\n49271.568421\n95\n\n\n\"New England\"\n73872.653846\n26\n\n\n\"East North Central\"\n58998.2875\n160\n\n\n\"Middle Atlantic\"\n62316.318182\n66\n\n\n\"South Atlantic\"\n52636.734568\n162\n\n\n\"West South Central\"\n52675.646154\n130\n\n\n\"West North Central\"\n60065.508333\n120\n\n\n\n\n\n\nFind the most populous metro area in each quadrant of the country.\n\n(\n    metro\n    .sort(c.pop, descending=True)\n    .group_by(c.quad)\n    .head(1)\n)\n\n\nshape: (5, 13)\n\n\n\nquad\nname\ngeoid\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\n\n\nstr\nstr\ni64\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\n\n\n\n\n\"NE\"\n\"New York\"\n35620\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n\n\n\"S\"\n\"Dallas\"\n19100\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n\n\n\"W\"\n\"Los Angeles\"\n31080\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n\n\n\"NC\"\n\"Chicago\"\n16980\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n\n\n\"O\"\n\"San Juan\"\n41980\n-66.112516\n18.27534\n2.096657\n554.930629\n41.8\n24755\n69.3\n379\n31.9\n\"South Atlantic\"\n\n\n\n\n\n\nThe population in our dataset is given in millions. Create a new column that shows the population in thousands instead.\n\n(\n    metro\n    .with_columns(\n        pop_thousands = c.pop * 1_000\n    )\n)\n\n\nshape: (934, 14)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\npop_thousands\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\nf64\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n20011.812\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n13202.558\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n9607.711\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n7543.34\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n7048.954\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n13.945\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n13.939\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n13.24\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n12.887\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n12.371\n\n\n\n\n\n\nCreate a new column that computes the annual rent for a one-bedroom apartment (the current rent column shows monthly rent).\n\n(\n    metro\n    .with_columns(\n        rent_1br_annual = c.rent_1br_median * 12\n    )\n)\n\n\nshape: (934, 14)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\nrent_1br_annual\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\ni64\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n17160\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n17616\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n12720\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n13272\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n11964\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n3852\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n11352\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n5820\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n6036\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n7332\n\n\n\n\n\n\nThe median household income is given as an exact figure, but it can be useful to round it. Create a new column that rounds the income to the nearest $10,000. Hint: Use the same integer division technique we used for computing decades from years.\n\n(\n    metro\n    .with_columns(\n        income_rounded = (c.hh_income_median // 10_000 * 10_000)\n    )\n)\n\n\nshape: (934, 14)\n\n\n\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\ndivision\nincome_rounded\n\n\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\nstr\ni64\n\n\n\n\n\"New York\"\n35620\n\"NE\"\n-74.101056\n40.76877\n20.011812\n1051.306467\n42.9\n86445\n55.3\n1430\n31.0\n\"Middle Atlantic\"\n80000\n\n\n\"Los Angeles\"\n31080\n\"W\"\n-118.148722\n34.219406\n13.202558\n1040.647281\n41.6\n81652\n51.3\n1468\n33.6\n\"Pacific\"\n80000\n\n\n\"Chicago\"\n16980\n\"NC\"\n-87.95882\n41.700605\n9.607711\n508.629406\n41.9\n78790\n68.9\n1060\n29.0\n\"East North Central\"\n70000\n\n\n\"Dallas\"\n19100\n\"S\"\n-96.970508\n32.84948\n7.54334\n323.181404\n41.3\n76916\n64.1\n1106\n29.1\n\"West South Central\"\n70000\n\n\n\"Houston\"\n26420\n\"S\"\n-95.401574\n29.787083\n7.048954\n316.543514\n41.0\n72551\n65.1\n997\n30.0\n\"West South Central\"\n70000\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\"West South Central\"\n30000\n\n\n\"Ketchikan\"\n28540\n\"O\"\n-130.927217\n55.582484\n0.013939\n1.046556\n44.6\n77820\n67.5\n946\n31.2\n\"Pacific\"\n70000\n\n\n\"Craig\"\n18780\n\"W\"\n-108.207523\n40.618749\n0.01324\n1.077471\n44.5\n58583\n72.8\n485\n29.0\n\"Mountain\"\n50000\n\n\n\"Vernon\"\n46900\n\"S\"\n-99.240853\n34.080611\n0.012887\n5.086411\n41.5\n45262\n62.0\n503\n22.0\n\"West South Central\"\n40000\n\n\n\"Lamesa\"\n29500\n\"S\"\n-101.947637\n32.742488\n0.012371\n5.291467\n41.1\n42778\n71.5\n611\n34.5\n\"West South Central\"\n40000\n\n\n\n\n\n\nNow, count how many metro areas fall into each income bracket (rounded to the nearest $10,000).\n\n(\n    metro\n    .with_columns(\n        income_rounded = (c.hh_income_median // 10_000 * 10_000)\n    )\n    .group_by(c.income_rounded)\n    .agg(\n        count = pl.len()\n    )\n    .sort(c.income_rounded)\n)\n\n\nshape: (13, 2)\n\n\n\nincome_rounded\ncount\n\n\ni64\nu32\n\n\n\n\n10000\n8\n\n\n20000\n2\n\n\n30000\n38\n\n\n40000\n168\n\n\n50000\n353\n\n\n…\n…\n\n\n90000\n11\n\n\n100000\n3\n\n\n110000\n2\n\n\n120000\n1\n\n\n130000\n1\n\n\n\n\n\n\nFor each census division, find the metro area with the lowest median rent. Sort the final result alphabetically by division name.\n\n(\n    metro\n    .sort(c.rent_1br_median)\n    .group_by(c.division)\n    .head(1)\n    .sort(c.division)\n)\n\n\nshape: (9, 13)\n\n\n\ndivision\nname\ngeoid\nquad\nlon\nlat\npop\ndensity\nage_median\nhh_income_median\npercent_own\nrent_1br_median\nrent_perc_income\n\n\nstr\nstr\ni64\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\ni64\nf64\n\n\n\n\n\"East North Central\"\n\"Scottsburg\"\n42500\n\"NC\"\n-85.747482\n38.685054\n0.02429\n48.696781\n41.5\n45794\n71.8\n356\n28.7\n\n\n\"East South Central\"\n\"Cleveland\"\n17380\n\"S\"\n-90.880722\n33.796866\n0.031301\n13.306982\n41.0\n33729\n56.8\n297\n35.1\n\n\n\"Middle Atlantic\"\n\"Johnstown\"\n27780\n\"NE\"\n-78.713799\n40.495051\n0.134048\n74.797763\n45.6\n51018\n79.4\n438\n27.6\n\n\n\"Mountain\"\n\"Española\"\n21580\n\"W\"\n-106.692234\n36.509588\n0.040347\n2.642677\n42.7\n46994\n79.7\n330\n26.4\n\n\n\"New England\"\n\"Berlin\"\n13620\n\"NE\"\n-71.305824\n44.688233\n0.03136\n6.629293\n47.4\n52054\n77.3\n553\n27.1\n\n\n\"Pacific\"\n\"Ontario\"\n36620\n\"W\"\n-117.589277\n43.218942\n0.056241\n2.103687\n42.9\n52486\n65.8\n538\n23.4\n\n\n\"South Atlantic\"\n\"Coco\"\n17640\n\"O\"\n-66.255037\n18.007166\n0.026208\n136.329824\n40.8\n18563\n76.1\n134\n24.3\n\n\n\"West North Central\"\n\"Kennett\"\n28380\n\"NC\"\n-90.091077\n36.271631\n0.028592\n20.176489\n42.4\n42194\n64.1\n369\n27.5\n\n\n\"West South Central\"\n\"Zapata\"\n49820\n\"S\"\n-99.168533\n27.000573\n0.013945\n5.081383\n41.4\n34406\n77.1\n321\n34.6\n\n\n\n\n\n\nFind the divisions that have more than 75 metro areas. Show the average home ownership rate for each of these divisions, sorted from highest to lowest ownership rate.\n\n(\n    metro\n    .group_by(c.division)\n    .agg(\n        percent_own_mean = c.percent_own.mean(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 75)\n    .sort(c.percent_own_mean, descending=True)\n)\n\n\nshape: (7, 3)\n\n\n\ndivision\npercent_own_mean\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"East North Central\"\n74.1825\n160\n\n\n\"West North Central\"\n71.874167\n120\n\n\n\"Mountain\"\n70.933333\n93\n\n\n\"East South Central\"\n69.44\n95\n\n\n\"South Atlantic\"\n68.708025\n162\n\n\n\"West South Central\"\n68.266923\n130\n\n\n\"Pacific\"\n64.862195\n82\n\n\n\n\n\n\nFor each division with more than 75 metro areas, show the name of the metro with the highest population density. Use print_rows to display all results.\n\n(\n    metro\n    .sort(c.density, descending=True)\n    .group_by(c.division)\n    .agg(\n        densest_metro = c.name.first(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 75)\n    .pipe(print_rows)\n)\n\n\nshape: (7, 3)\n\n\n\ndivision\ndensest_metro\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"Mountain\"\n\"Boulder\"\n93\n\n\n\"Pacific\"\n\"Los Angeles\"\n82\n\n\n\"East South Central\"\n\"Louisville/Jefferson County\"\n95\n\n\n\"East North Central\"\n\"Chicago\"\n160\n\n\n\"West South Central\"\n\"Dallas\"\n130\n\n\n\"South Atlantic\"\n\"San Juan\"\n162\n\n\n\"West North Central\"\n\"Minneapolis\"\n120",
    "crumbs": [
      "Notebook02d"
    ]
  },
  {
    "objectID": "notebook02b.html",
    "href": "notebook02b.html",
    "title": "Notebook02b",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\ncountry = pl.read_csv(ub + \"data/countries.csv\").drop(c.lang)\n\n\n\nQuestions\nIn this notebook, we will continue practicing data manipulation techniques using a dataset of countries. The country dataset contains information about various countries including their geographic coordinates (latitude and longitude), life expectancy, happiness scores, population, GDP per capita, and the region of the world they belong to.\nStart by finding the country with the highest life expectancy. Your result should show just that one row.\n\n(\n    country\n    .sort(c.lexp, descending=True)\n    .head(1)\n)\n\n\nshape: (1, 14)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SGP\"\n\"Singapore\"\n\"Asia\"\n\"South-eastern Asia\"\n5.871\n85.63\n1.3\n103.8\n0.946\n137906\nnull\n66.54\n145.5\n100.0\n\n\n\n\n\n\nNow, select all countries that are located in both the Northern Hemisphere (latitude greater than 0) and the Eastern Hemisphere (longitude greater than 0).\n\n# Can also combine into a single filter using `&`\n(\n    country\n    .filter(c.lat &gt; 0)\n    .filter(c.lon &gt; 0)\n)\n\n\nshape: (83, 14)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\n\n\"SGP\"\n\"Singapore\"\n\"Asia\"\n\"South-eastern Asia\"\n5.871\n85.63\n1.3\n103.8\n0.946\n137906\nnull\n66.54\n145.5\n100.0\n\n\n\"BGR\"\n\"Bulgaria\"\n\"Europe\"\n\"Eastern Europe\"\n6.715\n74.33\n42.75\n25.5\n0.845\n36211\n40.3\n55.9\n137.1\n86.00395\n\n\n\"AFG\"\n\"Afghanistan\"\n\"Asia\"\n\"Southern Asia\"\n43.844\n65.11\n33.0\n66.0\n0.496\n2093\n27.8\n14.46\n36.1\n47.47762\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"THA\"\n\"Thailand\"\n\"Asia\"\n\"South-eastern Asia\"\n71.62\n79.78\n14.0\n101.0\n0.798\n22696\n35.0\n62.82\n104.6\n97.79091\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\n\n\n\n\n\nNext, select all countries that are located in either the Northern Hemisphere (latitude greater than 0) or the Eastern Hemisphere (longitude greater than 0). This should return more countries than the previous question.\n\n(\n    country\n    .filter((c.lat &gt; 0) | (c.lon &gt; 0))\n)\n\n\nshape: (128, 14)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n\n\n\n\n\n\nSelect only the countries that are in Africa.\n\n(\n    country\n    .filter(c.region == \"Africa\")\n)\n\n\nshape: (37, 14)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\n\n\"GAB\"\n\"Gabon\"\n\"Africa\"\n\"Middle Africa\"\n2.593\n68.68\n-0.683331\n11.5\n0.733\n19543\n38.0\n51.04\n93.6\n49.20331\n\n\n\"TZA\"\n\"Tanzania, United Republic of\"\n\"Africa\"\n\"Eastern Africa\"\n70.546\n68.59\n-6.306944\n34.853889\n0.555\n3924\n40.5\n40.42\n46.9\n26.78297\n\n\n\"MWI\"\n\"Malawi\"\n\"Africa\"\n\"Eastern Africa\"\n22.216\n66.2\n-13.0\n34.0\n0.517\n1689\n38.5\n32.72\n21.0\n38.45707\n\n\n\"GHA\"\n\"Ghana\"\n\"Africa\"\n\"Western Africa\"\n35.064\n67.41\n7.9465\n-1.0232\n0.628\n7404\n43.5\n42.98\n68.4\n22.67011\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ETH\"\n\"Ethiopia\"\n\"Africa\"\n\"Eastern Africa\"\n135.472\n70.68\n9.0\n40.0\n0.497\n3107\n35.0\n40.93\n7.6\n8.04716\n\n\n\"CMR\"\n\"Cameroon\"\n\"Africa\"\n\"Middle Africa\"\n29.879\n64.52\n5.133333\n12.65\n0.588\n5107\n46.6\n49.46\n43.9\n41.90133\n\n\n\"LBR\"\n\"Liberia\"\n\"Africa\"\n\"Western Africa\"\n5.731\n67.61\n6.533333\n-9.75\n0.51\n1752\n35.3\n44.94\n38.7\n19.63094\n\n\n\"MAR\"\n\"Morocco\"\n\"Africa\"\n\"Northern Africa\"\n38.431\n74.71\n32.0\n-6.0\n0.71\n9583\n39.5\n44.87\n98.5\n84.47591\n\n\n\"GMB\"\n\"Gambia\"\n\"Africa\"\n\"Western Africa\"\n2.822\n68.16\n13.5\n-15.5\n0.524\n3199\nnull\n46.91\n76.7\n46.90722\n\n\n\n\n\n\nNow, select countries that are in either Africa or Europe. Note: Do this without using the | (or) operator.\n\n(\n    country\n    .filter(c.region.is_in([\"Africa\", \"Europe\"]))\n)\n\n\nshape: (75, 14)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\n\n\"GAB\"\n\"Gabon\"\n\"Africa\"\n\"Middle Africa\"\n2.593\n68.68\n-0.683331\n11.5\n0.733\n19543\n38.0\n51.04\n93.6\n49.20331\n\n\n\"BGR\"\n\"Bulgaria\"\n\"Europe\"\n\"Eastern Europe\"\n6.715\n74.33\n42.75\n25.5\n0.845\n36211\n40.3\n55.9\n137.1\n86.00395\n\n\n\"TZA\"\n\"Tanzania, United Republic of\"\n\"Africa\"\n\"Eastern Africa\"\n70.546\n68.59\n-6.306944\n34.853889\n0.555\n3924\n40.5\n40.42\n46.9\n26.78297\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"LUX\"\n\"Luxembourg\"\n\"Europe\"\n\"Western Europe\"\n0.68\n83.82\n49.77\n6.13\n0.922\n131038\n34.9\n70.16\n143.3\n97.60054\n\n\n\"LTU\"\n\"Lithuania\"\n\"Europe\"\n\"Northern Europe\"\n2.83\n77.19\n55.2\n24.0\n0.895\n49761\n35.3\n65.53\n157.9\n92.73568\n\n\n\"MNE\"\n\"Montenegro\"\n\"Europe\"\n\"Southern Europe\"\n0.633\n76.92\n42.766667\n19.216667\n0.862\n29492\n29.4\n58.13\n185.1\n96.657\n\n\n\"GMB\"\n\"Gambia\"\n\"Africa\"\n\"Western Africa\"\n2.822\n68.16\n13.5\n-15.5\n0.524\n3199\nnull\n46.91\n76.7\n46.90722\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n\n\n\n\n\n\nFind the happiest country in each region. That is, for each region, return only the country with the highest happiness score.\n\n(\n    country\n    .sort(c.happy, descending=True)\n    .group_by(c.region)\n    .head(1)\n)\n\n\nshape: (5, 14)\n\n\n\nregion\niso\nfull_name\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Asia\"\n\"KWT\"\n\"Kuwait\"\n\"Western Asia\"\n5.026\n82.62\n29.166667\n47.6\n0.852\n46572\nnull\n71.3\n135.2\n100.0\n\n\n\"Oceania\"\n\"AUS\"\n\"Australia\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n\n\n\"Americas\"\n\"CRI\"\n\"Costa Rica\"\n\"Central America\"\n5.153\n81.33\n10.0\n-84.0\n0.833\n28390\n49.3\n73.84\n68.7\n97.47361\n\n\n\"Africa\"\n\"LBY\"\n\"Libya\"\n\"Northern Africa\"\n7.459\n77.15\n27.0\n17.0\n0.721\n14636\nnull\n59.7\n167.7\n92.10514\n\n\n\"Europe\"\n\"FIN\"\n\"Finland\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n\n\n\n\n\n\nThe population column in our dataset is given in millions. Create a new column that contains the raw population (i.e., the actual number of people rather than millions of people).\n\n# Python allows, optionally, to use underscores in a number\n# \"as visual separators for digit grouping purposes\" (PEP 515)\n# these do not affect the subsequent code but make it much\n# easier to read large numbers\n(\n    country\n    .with_columns(\n        pop_raw = c.pop * 1_000_000\n    )\n)\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\npop_raw\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"SEN\"\n\"Senegal\"\n\"Africa\"\n\"Western Africa\"\n18.932\n70.43\n14.366667\n-14.283333\n0.53\n4871\n38.1\n50.93\n66.0\n54.93987\n1.8932e7\n\n\n\"VEN\"\n\"Venezuela, Bolivarian Republic…\n\"Americas\"\n\"South America\"\n28.517\n76.18\n8.0\n-67.0\n0.709\n8899\n44.8\n57.65\n96.8\n95.66913\n2.8517e7\n\n\n\"FIN\"\n\"Finland\"\n\"Europe\"\n\"Northern Europe\"\n5.623\n82.84\n65.0\n27.0\n0.948\n57574\n27.7\n76.99\n156.4\n99.44798\n5.623e6\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n3.47276e8\n\n\n\"LKA\"\n\"Sri Lanka\"\n\"Asia\"\n\"Southern Asia\"\n23.229\n78.51\n7.0\n81.0\n0.776\n14380\n39.3\n36.02\n83.1\n90.77437\n2.3229e7\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"ALB\"\n\"Albania\"\n\"Europe\"\n\"Southern Europe\"\n2.772\n79.67\n41.0\n20.0\n0.81\n20362\n30.8\n54.45\n91.9\n98.5473\n2.772e6\n\n\n\"MYS\"\n\"Malaysia\"\n\"Asia\"\n\"South-eastern Asia\"\n35.978\n76.03\n3.7805111\n102.314362\n0.819\n35990\n46.2\n58.68\n118.2\n95.69194\n3.5978e7\n\n\n\"SLV\"\n\"El Salvador\"\n\"Americas\"\n\"Central America\"\n6.366\n76.98\n13.668889\n-88.866111\n0.678\n12221\n38.3\n64.82\n126.9\n86.19786\n6.366e6\n\n\n\"CYP\"\n\"Cyprus\"\n\"Asia\"\n\"Western Asia\"\n1.371\n81.77\n35.0\n33.0\n0.913\n55720\n31.2\n60.71\n123.1\n99.41781\n1.371e6\n\n\n\"PAK\"\n\"Pakistan\"\n\"Asia\"\n\"Southern Asia\"\n255.22\n66.71\n30.0\n71.0\n0.544\n5717\n29.6\n45.49\n49.8\n61.92651\n2.5522e8\n\n\n\n\n\n\nThe GDP per capita of the United States (in the dataset) is 78_389. Compute the absolute distance using .abs of each country’s GDP to this value and sort the dataset by the distance.\n\n(\n    country\n    .with_columns(\n        gdp_dist = (c.gdp - 78_389).abs()\n    )\n    .sort(c.gdp_dist)\n)\n\n\nshape: (135, 15)\n\n\n\niso\nfull_name\nregion\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\ngdp_dist\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\ni64\n\n\n\n\n\"USA\"\n\"United States of America\"\n\"Americas\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n0\n\n\n\"DNK\"\n\"Denmark\"\n\"Europe\"\n\"Northern Europe\"\n6.003\n82.02\n56.0\n10.0\n0.962\n77337\n27.7\n75.04\n115.7\n99.59723\n1052\n\n\n\"NLD\"\n\"Netherlands, Kingdom of the\"\n\"Europe\"\n\"Western Europe\"\n18.347\n82.55\n52.366667\n4.883333\n0.955\n73384\nnull\n72.55\n114.4\n97.71353\n5005\n\n\n\"ARE\"\n\"United Arab Emirates\"\n\"Asia\"\n\"Western Asia\"\n11.346\n74.94\n24.4\n54.3\n0.94\n72660\n26.0\n67.28\n157.5\n99.10363\n5729\n\n\n\"CHE\"\n\"Switzerland\"\n\"Europe\"\n\"Western Europe\"\n8.967\n84.85\n46.798562\n8.231973\n0.97\n84311\n33.1\n69.69\n123.2\n99.89987\n5922\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"YEM\"\n\"Yemen\"\n\"Asia\"\n\"Western Asia\"\n41.774\n69.27\n15.5\n48.0\n0.47\n1722\n35.9\n35.32\n41.4\n52.61741\n76667\n\n\n\"MDG\"\n\"Madagascar\"\n\"Africa\"\n\"Eastern Africa\"\n32.741\n67.03\n-20.0\n47.0\n0.487\n1720\n42.6\n44.33\n34.8\n12.1276\n76669\n\n\n\"MWI\"\n\"Malawi\"\n\"Africa\"\n\"Eastern Africa\"\n22.216\n66.2\n-13.0\n34.0\n0.517\n1689\n38.5\n32.72\n21.0\n38.45707\n76700\n\n\n\"COD\"\n\"Congo, Democratic Republic of …\n\"Africa\"\n\"Middle Africa\"\n112.832\n66.25\n-2.88\n23.656111\n0.522\n1566\n42.1\n33.83\n17.2\n17.04284\n76823\n\n\n\"MOZ\"\n\"Mozambique\"\n\"Africa\"\n\"Eastern Africa\"\n35.632\n60.06\n-19.0\n35.0\n0.493\n1531\n54.0\n57.04\n31.4\n30.56033\n76858\n\n\n\n\n\n\nFind the country in each region that has the closest GDP per capita to the United States. Note: You will probably want to first copy your answer above and make changes rather than starting from scratch.\n\n(\n    country\n    .with_columns(\n        gdp_dist = (c.gdp - 78_389).abs()\n    )\n    .sort(c.gdp_dist)\n    .group_by(c.region)\n    .head(1)\n)\n\n\nshape: (5, 15)\n\n\n\nregion\niso\nfull_name\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\ngdp_dist\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\ni64\n\n\n\n\n\"Europe\"\n\"DNK\"\n\"Denmark\"\n\"Northern Europe\"\n6.003\n82.02\n56.0\n10.0\n0.962\n77337\n27.7\n75.04\n115.7\n99.59723\n1052\n\n\n\"Africa\"\n\"MUS\"\n\"Mauritius\"\n\"Eastern Africa\"\n1.268\n76.35\n-20.2\n57.5\n0.806\n28927\n36.8\n57.59\n92.8\n95.45876\n49462\n\n\n\"Oceania\"\n\"AUS\"\n\"Australia\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n16446\n\n\n\"Asia\"\n\"ARE\"\n\"United Arab Emirates\"\n\"Western Asia\"\n11.346\n74.94\n24.4\n54.3\n0.94\n72660\n26.0\n67.28\n157.5\n99.10363\n5729\n\n\n\"Americas\"\n\"USA\"\n\"United States of America\"\n\"Northern America\"\n347.276\n79.83\n39.828175\n-98.5795\n0.938\n78389\n47.7\n65.21\n91.7\n99.72235\n0\n\n\n\n\n\n\nThe United States is, of course, the closest to itself in “Americas”. Finally, modify the previous code to remove the United States from the data so that we can see what other country it is closest to in its own region. Note: When we say “modify” in these notes, that means to copy and then modify; don’t change the original code itself.\n\n(\n    country\n    .filter(c.iso != \"USA\")\n    .with_columns(\n        gdp_dist = (c.gdp - 78_389).abs()\n    )\n    .sort(c.gdp_dist)\n    .group_by(c.region)\n    .head(1)\n)\n\n\nshape: (5, 15)\n\n\n\nregion\niso\nfull_name\nsubregion\npop\nlexp\nlat\nlon\nhdi\ngdp\ngini\nhappy\ncellphone\nwater_access\ngdp_dist\n\n\nstr\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\nf64\ni64\n\n\n\n\n\"Africa\"\n\"MUS\"\n\"Mauritius\"\n\"Eastern Africa\"\n1.268\n76.35\n-20.2\n57.5\n0.806\n28927\n36.8\n57.59\n92.8\n95.45876\n49462\n\n\n\"Americas\"\n\"CAN\"\n\"Canada\"\n\"Northern America\"\n40.127\n83.15\n56.0\n-109.0\n0.939\n58422\n33.3\n68.41\n75.5\n98.92568\n19967\n\n\n\"Asia\"\n\"ARE\"\n\"United Arab Emirates\"\n\"Western Asia\"\n11.346\n74.94\n24.4\n54.3\n0.94\n72660\n26.0\n67.28\n157.5\n99.10363\n5729\n\n\n\"Oceania\"\n\"AUS\"\n\"Australia\"\n\"Australia and New Zealand\"\n26.974\n83.79\n-25.0\n133.0\n0.958\n61943\n32.4\n70.25\n101.6\n99.99923\n16446\n\n\n\"Europe\"\n\"DNK\"\n\"Denmark\"\n\"Northern Europe\"\n6.003\n82.02\n56.0\n10.0\n0.962\n77337\n27.7\n75.04\n115.7\n99.59723\n1052\n\n\n\n\n\n\nAnd there we go, already a fairly intricate manipulation of the data with just a few simple functions!",
    "crumbs": [
      "Notebook02b"
    ]
  },
  {
    "objectID": "notebook01b.html",
    "href": "notebook01b.html",
    "title": "Notebook01b",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\").drop(c.description)\n\n\n\nQuestions\nLet’s look at the food dataset that we loaded above. To get an idea about its structure, write the name of the dataset in the block below and run the cell with nothing else. This will show the column names and data types along with the first five and last five rows.\n\nfood\n\n\nshape: (61, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Apple\"\n\"fruit\"\n52\n0.1\n0.028\n0\n1\n13.81\n2.4\n10.39\n0.26\n1\n1\n8\n\"apple\"\n\"red\"\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Avocado\"\n\"fruit\"\n160\n14.6\n2.126\n0\n7\n8.53\n6.7\n0.66\n2.0\n3\n3\n17\n\"avocado\"\n\"green\"\n\n\n\"Banana\"\n\"fruit\"\n89\n0.3\n0.112\n0\n1\n22.84\n2.6\n12.23\n1.09\n1\n1\n15\n\"banana\"\n\"yellow\"\n\n\n\"Chickpea\"\n\"grains\"\n180\n2.9\n0.309\n0\n243\n29.98\n8.6\n5.29\n9.54\n17\n0\n3\n\"chickpea\"\n\"brown\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Tomato\"\n\"vegetable\"\n18\n0.0\n0.046\n0\n5\n3.92\n1.2\n2.63\n0.88\n2\n17\n21\n\"tomato\"\n\"red\"\n\n\n\"Tuna\"\n\"fish\"\n153\n3.9\n0.811\n53\n366\n0.41\n0.0\n0.09\n27.3\n5\n3\n4\n\"tuna\"\n\"red\"\n\n\n\"Turkey\"\n\"meat\"\n187\n7.0\n1.999\n77\n69\n0.0\n0.0\n0.0\n28.9\n8\n0\n0\n\"turkey_(bird)\"\n\"white\"\n\n\n\"Potato\"\n\"vegetable\"\n104\n2.0\n0.458\n0\n254\n19.36\n1.7\n0.82\n1.66\n2\n2\n12\n\"potato\"\n\"white\"\n\n\n\"Yogurt\"\n\"dairy\"\n99\n1.1\n0.742\n5\n53\n18.64\n0.0\n18.64\n3.98\n0\n1\n1\n\"yogurt\"\n\"white\"\n\n\n\n\n\n\nWhat kinds of information in this dataset are stored as numbers?\nAnswer: All of the nutritional data is stored as numbers (calories, total_fat, sat_fat, cholesterol, sodium, carbs, fiber, sugar, protein, iron, vitamin_a, vitamin_c).\nWhat variables in this dataset are stored as strings?\nAnswer: The food name (item), the food group, a string called wiki, and a color.\nHow many observations are there in the dataset?\nAnswer: There are 61 observations. We can see that in the shape parameter printed out above the DataFrame.\nCan you figure out how the rows of the data are ordered?\nAnswer: Based on the first and last rows, it appears to be alphabetically by the food item name (starts with Apple and three other A’s, then Banana and Chickpea, ending with Yogurt).\nThroughout this book, we will want to apply data manipulation, visualization, and modeling functions to data. Whenever possible, which is most of the time, we will write these are method chains in a very specific format. The first line of a code block starts with an opening parenthesis and the final line has the closing parenthesis. Inside, each of the steps of the analysis will get placed with four leading spaces (you should be able to hit tab in Colab to get this), with one step on each line. The dataset goes on its own line.\nIn the code below, greate a method chain that starts with food (on its own line), applies the method .head(10) to grab the first ten rows, and the applies the method .tail(3) to grab the last three rows the remaining data. Hint: The code should have five total lines.\n\n(\n    food\n    .head(10)\n    .tail(3)\n)\n\n\nshape: (3, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Bell Pepper\"\n\"vegetable\"\n26\n0.0\n0.059\n0\n2\n6.03\n2.0\n4.2\n0.99\n2\n63\n317\n\"bell_pepper\"\n\"green\"\n\n\n\"Crab\"\n\"fish\"\n87\n1.0\n0.222\n78\n293\n0.04\n0.0\n0.0\n18.06\n4\n0\n5\n\"callinectes_sapidus\"\n\"red\"\n\n\n\"Broccoli\"\n\"vegetable\"\n34\n0.3\n0.039\n0\n33\n6.64\n2.6\n1.7\n2.82\n4\n12\n149\n\"broccoli\"\n\"green\"\n\n\n\n\n\n\nWhat rows, relatively to the original dataset, are returned by the sequence of steps above?\nAnswer: We have the 8th, 9th, and 10th rows from the original dataset. This comes from first taking off the first 10 rows with head and then grabbing the last 3 rows of the result with tail.\nNote that method chains never modify the original dataset. After running the code above food still has all of the original data and the new version you printed out is no longer available. If we want to save a copy of the output, we need to add something like new_name = before the paranthesis on the first line of the code. When we do this, the original dataset will remain but we have a copy in the new_name object. In the code below, repeat the steps we did above with food but create a new object food_sub that consists of the subset of three rows.\n\nfood_sub = (\n    food\n    .head(10)\n    .tail(3)\n)\n\nNotice that we do not get to see the output of the operation above. Python does not print out the result because we saved it as a new variable. To see the result, we would need to write the name of the dataset as an extra row in the code block all on its own. This, for example, is how we printed the food dataset in the first question above. Do this below, recreating the food_sub and then including the name food_sub on its own line to see the data.\n\nfood_sub = (\n    food\n    .head(10)\n    .tail(3)\n)\nfood_sub\n\n\nshape: (3, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Bell Pepper\"\n\"vegetable\"\n26\n0.0\n0.059\n0\n2\n6.03\n2.0\n4.2\n0.99\n2\n63\n317\n\"bell_pepper\"\n\"green\"\n\n\n\"Crab\"\n\"fish\"\n87\n1.0\n0.222\n78\n293\n0.04\n0.0\n0.0\n18.06\n4\n0\n5\n\"callinectes_sapidus\"\n\"red\"\n\n\n\"Broccoli\"\n\"vegetable\"\n34\n0.3\n0.039\n0\n33\n6.64\n2.6\n1.7\n2.82\n4\n12\n149\n\"broccoli\"\n\"green\"\n\n\n\n\n\n\nIn these notebooks, please only save the output with a new object name if specifically instructed to do so. When asked to create a new object, include the object name as an extra line at the end so that we can also see and understand the results.",
    "crumbs": [
      "Notebook01b"
    ]
  },
  {
    "objectID": "notebook01a.html",
    "href": "notebook01a.html",
    "title": "Notebook01a",
    "section": "",
    "text": "This is our first Python notebook! The idea is just to have something to show (hopefully) how easy it is to get up and running in Python using Colab. Anyone can open this file and see the code. In order to run it, you’ll need to be signed into a Google account.\n\nSetup\nEvery notebook will start with some setup code. Here, the first block downloads the class-specific helper functions, the second loads all of the Python modules we need, and the final one downloads the data that we are using in this notebook. We won’t actually be using the data this time, but I’ve included it here just to demonstrate the normal process.\nRun each of these cells in order by clicking on the triangular play button to the left when hoving on the cell. Wait for each to finish before continuing.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\")\n\n\n\nQuestions\nMost notebooks will consist of blank blocks of code. There will be questions asking you to do certain things by writing code and then running it within each of the blocks. Sometimes, there will also be another question and corresponding Answer: tag after the code that asks you to provide a short textual response. You can type this directly after the tag. Let’s try this out! In the block below, use Python to compute the sum of two plus two.\n\n2 + 2\n\n4\n\n\nWhat is the sum?\nAnswer: Four, of course!\nAs in the notes, create a variable called my_fav_number equal to your favorite number. Note: See the font that the variable is in? Those come from putting the variable in back quotes within the text. Those are used to indicate that something in the prose text is a Python variable, function, or module. You will see that in the solutions these get printed in a code-font as well.\n\n# the solutions often show possible answers rather than\n# the only answer; also, there may be comments such as this\n# that start with the # symbol and provide additional \n# feedback on the answer\nmy_fav_number = 8  \n\nNow, use the function np.log, a function that comes from the numpy module to compute logarithms, to find the logarithm of 10.\n\nnp.log(10)\n\nnp.float64(2.302585092994046)\n\n\nIf you are less mathematically-oriented, you might be worried that we are starting with a lot of mathematics? Don’t be! This is only because it’s the easiest thing to do before we have learned any of the data-specific functions that will come up in the next chapter. If you are very mathematically-oriented, you may be worried about by why the logarithm of 10 is not 1. The reason for this is the Python, like most programming languages but unlike most mathematical texts, uses the term log to indicate the natural logarithm (i.e., base e).\n\n\nColab Setup\nWe have seen in the examples above that we can run Python code in Colab with no Colab-specific setup required. There are a few things that will be helpful to change in the default settings going forward. If you set these once from your Google account, they should persist without having the reset them each time. To start, click on Tools and Settings to open the settings box. Then:\n\nUnder Editor change “Indentation width in spaces” to 4.\nUnder Editor, unclick “Automatically close brackets…”\nUnder Editor, under “Code Diagnostics” select “None”\nUnder AI Assistance, click the “Hide generative AI features” and unclick everything else (very important!)\n\nThere are also two very important keyboard shortcuts to know:\n\nRun selected cell in place: Cmd + Enter (macOS), Ctrl + Enter (Windows)\nRun cell and move to next cell: Shift + Enter (macOS and Windows)\n\nA few other things are helpful to know about Colab. Under File, you can click on “Save a Copy in Drive”. This will save your work in a special Colab folder within your Google Drive. Otherwise, you will loss all of your work when closing the window.\nIf you reopen the Notebook from your Google Drive, it will use your default language settings, which may be difficult to use if it is not in English. You can change this by selecting Help (always the last item in the menu) and “Show in English” (always the last item in the pull-down menu).\nFinally, it seems that Google has been giving free Colab Pro accounts to students and educators. If you get a pop-up about this, it is worth going through the steps. While Colab is free, you’ll get more powerful options for running code with the Pro version, which could be useful (but not necessary) in the later chapters.",
    "crumbs": [
      "Notebook01a"
    ]
  },
  {
    "objectID": "notebook02a.html",
    "href": "notebook02a.html",
    "title": "Notebook02a",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\").drop(c.description)\n\n\n\nQuestions\nWe are going to do a number of data manipulation exercises here. Apologies that, because we are just getting started, the number of complete analyses and insights we will gain at this point are limited. We will get there soon! We will work with the food dataset again. Remember that in all questions you should just print out the results; don’t save them unless ask.\nStart by sorting the food data by calories, with the lowest caloric foods at the top.\n\n(\n    food\n    .sort(c.calories)\n)\n\n\nshape: (61, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Cucumber\"\n\"vegetable\"\n12\n0.1\n0.013\n0\n2\n2.16\n0.7\n1.38\n0.59\n1\n1\n5\n\"cucumber\"\n\"green\"\n\n\n\"Celery\"\n\"vegetable\"\n14\n0.1\n0.043\n0\n80\n2.97\n1.6\n1.83\n0.69\n1\n9\n5\n\"celery\"\n\"green\"\n\n\n\"Lettuce\"\n\"vegetable\"\n14\n0.1\n0.018\n0\n10\n2.97\n1.2\n1.76\n0.9\n2\n10\n5\n\"lettuce\"\n\"green\"\n\n\n\"Tomato\"\n\"vegetable\"\n18\n0.0\n0.046\n0\n5\n3.92\n1.2\n2.63\n0.88\n2\n17\n21\n\"tomato\"\n\"red\"\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Pork\"\n\"meat\"\n271\n17.0\n6.168\n90\n384\n0.0\n0.0\n0.0\n27.34\n6\n1\n1\n\"pork\"\n\"white\"\n\n\n\"Beef\"\n\"meat\"\n288\n19.5\n7.731\n87\n384\n0.0\n0.0\n0.0\n26.33\n15\n0\n0\n\"beef\"\n\"red\"\n\n\n\"Lamb\"\n\"meat\"\n292\n20.7\n8.756\n96\n394\n0.0\n0.0\n0.0\n24.32\n10\n0\n0\n\"sheep\"\n\"red\"\n\n\n\"Cheese\"\n\"dairy\"\n350\n26.9\n16.609\n83\n955\n4.71\n0.0\n3.54\n22.21\n3\n16\n0\n\"cheese\"\n\"yellow\"\n\n\n\"Oat\"\n\"grains\"\n389\n6.0\n1.217\n0\n2\n66.27\n10.6\n0.0\n16.89\n26\n0\n0\n\"oat\"\n\"brown\"\n\n\n\n\n\n\nNext, sort by calories but with the highest caloric foods at the top.\n\n(\n    food\n    .sort(c.calories, descending=True)\n)\n\n\nshape: (61, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Oat\"\n\"grains\"\n389\n6.0\n1.217\n0\n2\n66.27\n10.6\n0.0\n16.89\n26\n0\n0\n\"oat\"\n\"brown\"\n\n\n\"Cheese\"\n\"dairy\"\n350\n26.9\n16.609\n83\n955\n4.71\n0.0\n3.54\n22.21\n3\n16\n0\n\"cheese\"\n\"yellow\"\n\n\n\"Lamb\"\n\"meat\"\n292\n20.7\n8.756\n96\n394\n0.0\n0.0\n0.0\n24.32\n10\n0\n0\n\"sheep\"\n\"red\"\n\n\n\"Beef\"\n\"meat\"\n288\n19.5\n7.731\n87\n384\n0.0\n0.0\n0.0\n26.33\n15\n0\n0\n\"beef\"\n\"red\"\n\n\n\"Pork\"\n\"meat\"\n271\n17.0\n6.168\n90\n384\n0.0\n0.0\n0.0\n27.34\n6\n1\n1\n\"pork\"\n\"white\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Tomato\"\n\"vegetable\"\n18\n0.0\n0.046\n0\n5\n3.92\n1.2\n2.63\n0.88\n2\n17\n21\n\"tomato\"\n\"red\"\n\n\n\"Celery\"\n\"vegetable\"\n14\n0.1\n0.043\n0\n80\n2.97\n1.6\n1.83\n0.69\n1\n9\n5\n\"celery\"\n\"green\"\n\n\n\"Lettuce\"\n\"vegetable\"\n14\n0.1\n0.018\n0\n10\n2.97\n1.2\n1.76\n0.9\n2\n10\n5\n\"lettuce\"\n\"green\"\n\n\n\"Cucumber\"\n\"vegetable\"\n12\n0.1\n0.013\n0\n2\n2.16\n0.7\n1.38\n0.59\n1\n1\n5\n\"cucumber\"\n\"green\"\n\n\n\n\n\n\nAnd now, sort by the food groups (alphabetical order) and then within each food group by calories. Note: When we say “sort” in general, that means ascending unless otherwise noted.\n\n(\n    food\n    .sort(c.food_group, c.calories)\n)\n\n\nshape: (61, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Milk\"\n\"dairy\"\n60\n3.2\n1.865\n10\n40\n4.52\n0.0\n5.26\n3.22\n0\n2\n0\n\"milk\"\n\"white\"\n\n\n\"Yogurt\"\n\"dairy\"\n99\n1.1\n0.742\n5\n53\n18.64\n0.0\n18.64\n3.98\n0\n1\n1\n\"yogurt\"\n\"white\"\n\n\n\"Sour Cream\"\n\"dairy\"\n214\n20.9\n13.047\n44\n53\n4.27\n0.0\n0.16\n3.16\n0\n13\n2\n\"sour_cream\"\n\"white\"\n\n\n\"Cheese\"\n\"dairy\"\n350\n26.9\n16.609\n83\n955\n4.71\n0.0\n3.54\n22.21\n3\n16\n0\n\"cheese\"\n\"yellow\"\n\n\n\"Crab\"\n\"fish\"\n87\n1.0\n0.222\n78\n293\n0.04\n0.0\n0.0\n18.06\n4\n0\n5\n\"callinectes_sapidus\"\n\"red\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Carrot\"\n\"vegetable\"\n41\n0.2\n0.037\n0\n69\n9.58\n2.8\n4.54\n0.93\n2\n336\n10\n\"carrot\"\n\"orange\"\n\n\n\"Onion\"\n\"vegetable\"\n42\n0.0\n0.026\n0\n3\n10.11\n1.4\n4.28\n0.92\n1\n0\n11\n\"onion\"\n\"white\"\n\n\n\"Corn\"\n\"vegetable\"\n86\n1.1\n0.182\n0\n15\n19.02\n2.7\n3.22\n3.22\n3\n4\n11\n\"maize\"\n\"yellow\"\n\n\n\"Sweet Potato\"\n\"vegetable\"\n86\n0.0\n0.018\n0\n55\n20.12\n3.0\n4.18\n1.57\n3\n284\n4\n\"sweet_potato\"\n\"orange\"\n\n\n\"Potato\"\n\"vegetable\"\n104\n2.0\n0.458\n0\n254\n19.36\n1.7\n0.82\n1.66\n2\n2\n12\n\"potato\"\n\"white\"\n\n\n\n\n\n\nNow, take only the columns food group and calories from the dataset. Notice how close your code (should be) to the code in the previous block.\n\n(\n    food\n    .select(c.food_group, c.calories)\n)\n\n\nshape: (61, 2)\n\n\n\nfood_group\ncalories\n\n\nstr\ni64\n\n\n\n\n\"fruit\"\n52\n\n\n\"vegetable\"\n20\n\n\n\"fruit\"\n160\n\n\n\"fruit\"\n89\n\n\n\"grains\"\n180\n\n\n…\n…\n\n\n\"vegetable\"\n18\n\n\n\"fish\"\n153\n\n\n\"meat\"\n187\n\n\n\"vegetable\"\n104\n\n\n\"dairy\"\n99\n\n\n\n\n\n\nLet’s now chain several methods together. Create a dataset that has the ten items that have the largest amount of calories per serving.\n\n(\n    food\n    .sort(c.calories, descending=True)\n    .head(n=10)\n)\n\n\nshape: (10, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Oat\"\n\"grains\"\n389\n6.0\n1.217\n0\n2\n66.27\n10.6\n0.0\n16.89\n26\n0\n0\n\"oat\"\n\"brown\"\n\n\n\"Cheese\"\n\"dairy\"\n350\n26.9\n16.609\n83\n955\n4.71\n0.0\n3.54\n22.21\n3\n16\n0\n\"cheese\"\n\"yellow\"\n\n\n\"Lamb\"\n\"meat\"\n292\n20.7\n8.756\n96\n394\n0.0\n0.0\n0.0\n24.32\n10\n0\n0\n\"sheep\"\n\"red\"\n\n\n\"Beef\"\n\"meat\"\n288\n19.5\n7.731\n87\n384\n0.0\n0.0\n0.0\n26.33\n15\n0\n0\n\"beef\"\n\"red\"\n\n\n\"Pork\"\n\"meat\"\n271\n17.0\n6.168\n90\n384\n0.0\n0.0\n0.0\n27.34\n6\n1\n1\n\"pork\"\n\"white\"\n\n\n\"Catfish\"\n\"fish\"\n240\n14.5\n3.246\n69\n398\n8.54\n0.5\n0.85\n17.57\n6\n1\n1\n\"catfish\"\n\"white\"\n\n\n\"Halibut\"\n\"fish\"\n239\n17.7\n3.102\n59\n103\n0.0\n0.0\n0.0\n18.42\n5\n1\n0\n\"halibut\"\n\"white\"\n\n\n\"Chicken\"\n\"meat\"\n237\n13.4\n3.758\n87\n404\n0.0\n0.0\n0.0\n27.07\n7\n3\n0\n\"chicken\"\n\"white\"\n\n\n\"Scallop\"\n\"fish\"\n217\n10.9\n2.216\n54\n487\n10.49\n0.5\n0.82\n18.14\n5\n2\n4\n\"scallop\"\n\"white\"\n\n\n\"Sour Cream\"\n\"dairy\"\n214\n20.9\n13.047\n44\n53\n4.27\n0.0\n0.16\n3.16\n0\n13\n2\n\"sour_cream\"\n\"white\"\n\n\n\n\n\n\nAnd now, in the next block of code grab just one row of the data that has the smallest amount of sugar amongst the 10 highest caloric foods.\n\n(\n    food\n    .sort(c.calories, descending=True)\n    .head(n=10)\n    .sort(c.sugar)\n    .head(n=1)\n)\n\n\nshape: (1, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Oat\"\n\"grains\"\n389\n6.0\n1.217\n0\n2\n66.27\n10.6\n0.0\n16.89\n26\n0\n0\n\"oat\"\n\"brown\"\n\n\n\n\n\n\nNow, select just those rows where the amount of iron is more than 3mg.\n\n(\n    food\n    .filter(c.iron &gt; 3)\n)\n\n\nshape: (25, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Chickpea\"\n\"grains\"\n180\n2.9\n0.309\n0\n243\n29.98\n8.6\n5.29\n9.54\n17\n0\n3\n\"chickpea\"\n\"brown\"\n\n\n\"String Bean\"\n\"vegetable\"\n31\n0.1\n0.026\n0\n6\n7.13\n3.4\n1.4\n1.82\n6\n14\n27\n\"green_bean\"\n\"green\"\n\n\n\"Beef\"\n\"meat\"\n288\n19.5\n7.731\n87\n384\n0.0\n0.0\n0.0\n26.33\n15\n0\n0\n\"beef\"\n\"red\"\n\n\n\"Crab\"\n\"fish\"\n87\n1.0\n0.222\n78\n293\n0.04\n0.0\n0.0\n18.06\n4\n0\n5\n\"callinectes_sapidus\"\n\"red\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Scallop\"\n\"fish\"\n217\n10.9\n2.216\n54\n487\n10.49\n0.5\n0.82\n18.14\n5\n2\n4\n\"scallop\"\n\"white\"\n\n\n\"Shrimp\"\n\"fish\"\n144\n2.3\n0.446\n206\n613\n1.24\n0.0\n0.0\n27.59\n16\n4\n3\n\"shrimp\"\n\"pink\"\n\n\n\"Swordfish\"\n\"fish\"\n177\n8.2\n1.959\n47\n494\n0.49\n0.0\n0.11\n23.8\n5\n5\n5\n\"swordfish\"\n\"white\"\n\n\n\"Tuna\"\n\"fish\"\n153\n3.9\n0.811\n53\n366\n0.41\n0.0\n0.09\n27.3\n5\n3\n4\n\"tuna\"\n\"red\"\n\n\n\"Turkey\"\n\"meat\"\n187\n7.0\n1.999\n77\n69\n0.0\n0.0\n0.0\n28.9\n8\n0\n0\n\"turkey_(bird)\"\n\"white\"\n\n\n\n\n\n\nNow, instead, select those rows where the amount of iron is more than or equal to 3mg.\n\n(\n    food\n    .filter(c.iron &gt;= 3)\n)\n\n\nshape: (33, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Avocado\"\n\"fruit\"\n160\n14.6\n2.126\n0\n7\n8.53\n6.7\n0.66\n2.0\n3\n3\n17\n\"avocado\"\n\"green\"\n\n\n\"Chickpea\"\n\"grains\"\n180\n2.9\n0.309\n0\n243\n29.98\n8.6\n5.29\n9.54\n17\n0\n3\n\"chickpea\"\n\"brown\"\n\n\n\"String Bean\"\n\"vegetable\"\n31\n0.1\n0.026\n0\n6\n7.13\n3.4\n1.4\n1.82\n6\n14\n27\n\"green_bean\"\n\"green\"\n\n\n\"Beef\"\n\"meat\"\n288\n19.5\n7.731\n87\n384\n0.0\n0.0\n0.0\n26.33\n15\n0\n0\n\"beef\"\n\"red\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Shrimp\"\n\"fish\"\n144\n2.3\n0.446\n206\n613\n1.24\n0.0\n0.0\n27.59\n16\n4\n3\n\"shrimp\"\n\"pink\"\n\n\n\"Sweet Potato\"\n\"vegetable\"\n86\n0.0\n0.018\n0\n55\n20.12\n3.0\n4.18\n1.57\n3\n284\n4\n\"sweet_potato\"\n\"orange\"\n\n\n\"Swordfish\"\n\"fish\"\n177\n8.2\n1.959\n47\n494\n0.49\n0.0\n0.11\n23.8\n5\n5\n5\n\"swordfish\"\n\"white\"\n\n\n\"Tuna\"\n\"fish\"\n153\n3.9\n0.811\n53\n366\n0.41\n0.0\n0.09\n27.3\n5\n3\n4\n\"tuna\"\n\"red\"\n\n\n\"Turkey\"\n\"meat\"\n187\n7.0\n1.999\n77\n69\n0.0\n0.0\n0.0\n28.9\n8\n0\n0\n\"turkey_(bird)\"\n\"white\"\n\n\n\n\n\n\nSelect those rows where the food group is “fruit”.\n\n(\n    food\n    .filter(c.iron == 5)\n    .filter(c.food_group == \"fruit\")\n)\n\n\nshape: (0, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\n\n\n\nSelect those rows where the food group is “fruit” and the iron is great than 3. Do this with two different calls to .filter.\n\n(\n    food\n    .filter(c.iron == 5)\n    .filter(c.food_group == \"fruit\")\n)\n\n\nshape: (0, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\n\n\n\nRepeat the previous question with a single .filter by using the & symbol.\n\n(\n    food\n    .filter((c.iron == 5) & (c.food_group == \"fruit\"))\n)\n\n\nshape: (0, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\n\n\n\nSelect the rows where the food group is either “fruit” or “vegetable” using the | (or) operator.\n\n(\n    food\n    .filter((c.food_group == \"fruit\") | (c.food_group == \"vegetable\"))\n)\n\n\nshape: (32, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Apple\"\n\"fruit\"\n52\n0.1\n0.028\n0\n1\n13.81\n2.4\n10.39\n0.26\n1\n1\n8\n\"apple\"\n\"red\"\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Avocado\"\n\"fruit\"\n160\n14.6\n2.126\n0\n7\n8.53\n6.7\n0.66\n2.0\n3\n3\n17\n\"avocado\"\n\"green\"\n\n\n\"Banana\"\n\"fruit\"\n89\n0.3\n0.112\n0\n1\n22.84\n2.6\n12.23\n1.09\n1\n1\n15\n\"banana\"\n\"yellow\"\n\n\n\"String Bean\"\n\"vegetable\"\n31\n0.1\n0.026\n0\n6\n7.13\n3.4\n1.4\n1.82\n6\n14\n27\n\"green_bean\"\n\"green\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Strawberry\"\n\"fruit\"\n32\n0.0\n0.015\n0\n1\n7.68\n2.0\n4.66\n0.67\n2\n0\n98\n\"strawberry\"\n\"red\"\n\n\n\"Sweet Potato\"\n\"vegetable\"\n86\n0.0\n0.018\n0\n55\n20.12\n3.0\n4.18\n1.57\n3\n284\n4\n\"sweet_potato\"\n\"orange\"\n\n\n\"Tangerine\"\n\"fruit\"\n53\n0.3\n0.039\n0\n2\n13.34\n1.8\n10.58\n0.81\n1\n14\n45\n\"tangerine\"\n\"orange\"\n\n\n\"Tomato\"\n\"vegetable\"\n18\n0.0\n0.046\n0\n5\n3.92\n1.2\n2.63\n0.88\n2\n17\n21\n\"tomato\"\n\"red\"\n\n\n\"Potato\"\n\"vegetable\"\n104\n2.0\n0.458\n0\n254\n19.36\n1.7\n0.82\n1.66\n2\n2\n12\n\"potato\"\n\"white\"\n\n\n\n\n\n\nWhat would happen if we replaced the | operator with an & (and)?\nAnswer: This would result in no rows, because nothing can be fruit AND vegetable. Notice that this can be confusing because it does not match common English conventions. For example, “bring me all of the fruits and vegetables from the bag” means bring all items that are (individually) either fruits or vegetables.\nFinally, select the rows that are fruits or vegetables using the .is_in method.\n\n(\n    food\n    .filter(c.food_group.is_in([\"fruit\", \"vegetable\"]))\n)\n\n\nshape: (32, 16)\n\n\n\nitem\nfood_group\ncalories\ntotal_fat\nsat_fat\ncholesterol\nsodium\ncarbs\nfiber\nsugar\nprotein\niron\nvitamin_a\nvitamin_c\nwiki\ncolor\n\n\nstr\nstr\ni64\nf64\nf64\ni64\ni64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nstr\nstr\n\n\n\n\n\"Apple\"\n\"fruit\"\n52\n0.1\n0.028\n0\n1\n13.81\n2.4\n10.39\n0.26\n1\n1\n8\n\"apple\"\n\"red\"\n\n\n\"Asparagus\"\n\"vegetable\"\n20\n0.1\n0.046\n0\n2\n3.88\n2.1\n1.88\n2.2\n12\n15\n9\n\"asparagus\"\n\"green\"\n\n\n\"Avocado\"\n\"fruit\"\n160\n14.6\n2.126\n0\n7\n8.53\n6.7\n0.66\n2.0\n3\n3\n17\n\"avocado\"\n\"green\"\n\n\n\"Banana\"\n\"fruit\"\n89\n0.3\n0.112\n0\n1\n22.84\n2.6\n12.23\n1.09\n1\n1\n15\n\"banana\"\n\"yellow\"\n\n\n\"String Bean\"\n\"vegetable\"\n31\n0.1\n0.026\n0\n6\n7.13\n3.4\n1.4\n1.82\n6\n14\n27\n\"green_bean\"\n\"green\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Strawberry\"\n\"fruit\"\n32\n0.0\n0.015\n0\n1\n7.68\n2.0\n4.66\n0.67\n2\n0\n98\n\"strawberry\"\n\"red\"\n\n\n\"Sweet Potato\"\n\"vegetable\"\n86\n0.0\n0.018\n0\n55\n20.12\n3.0\n4.18\n1.57\n3\n284\n4\n\"sweet_potato\"\n\"orange\"\n\n\n\"Tangerine\"\n\"fruit\"\n53\n0.3\n0.039\n0\n2\n13.34\n1.8\n10.58\n0.81\n1\n14\n45\n\"tangerine\"\n\"orange\"\n\n\n\"Tomato\"\n\"vegetable\"\n18\n0.0\n0.046\n0\n5\n3.92\n1.2\n2.63\n0.88\n2\n17\n21\n\"tomato\"\n\"red\"\n\n\n\"Potato\"\n\"vegetable\"\n104\n2.0\n0.458\n0\n254\n19.36\n1.7\n0.82\n1.66\n2\n2\n12\n\"potato\"\n\"white\"\n\n\n\n\n\n\nThis last one is the preferred way to do this as it generalizes better to more categories and more clearly maps onto the way we commonly thing about being in a set. The | operator is very useful for other kinds of selections, such as selecting all of the vegetables OR foods with no saturated fats.",
    "crumbs": [
      "Notebook02a"
    ]
  },
  {
    "objectID": "notebook02c.html",
    "href": "notebook02c.html",
    "title": "Notebook02c",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfilm = pl.read_csv(ub + \"data/criterion.csv\").filter(~c.rating_imdb.is_null())\n\n\n\nQuestions\nIn this notebook, we will practice data manipulation using a dataset of films from the Criterion Collection. The film dataset contains information about each movie including its title, director, year of release, runtime, and IMDB rating.\nStart by sorting the films by their IMDB rating, with the lowest-rated films at the top.\n\n(\n    film\n    .sort(c.rating_imdb)\n)\n\n\nshape: (1_470, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0062492\"\n\"Wild 90\"\n1968\n\"English\"\n\"Comedy|Crime|Drama\"\n\"Norman Mailer\"\n\"D.A. Pennebaker\"\n\"United States\"\n139\n3.5\nnull\nnull\n90\n20765561\n\"1968 film by Norman Mailer\"\n\"Wild 90 is a 1968 experimental…\nnull\nnull\n\n\n\"tt0064373\"\n\"All Monsters Attack\"\n1969\n\"Japanese\"\n\"Adventure|Family|Fantasy\"\n\"Ishirô Honda|Jun Fukuda|Kengo …\n\"Shin'ichi Sekizawa\"\n\"Japan|United States\"\n5763\n3.9\n38\nnull\n69\n570240\n\"1969 film by Ishirō Honda\"\n\"All Monsters Attack  is a 1969…\nnull\nnull\n\n\n\"tt0063195\"\n\"Genocide\"\n1968\n\"Japanese\"\n\"Horror|Sci-Fi\"\n\"Kazui Nihonmatsu\"\n\"Kingen Amada|Susumu Takaku\"\n\"Japan\"\n802\n4.7\nnull\nnull\n84\n51448466\n\"1968 Japanese film\"\n\"Genocide  is a 1968 Japanese s…\nnull\nnull\n\n\n\"tt0064625\"\n\"Maidstone\"\n1970\n\"English\"\n\"Drama\"\n\"Norman Mailer\"\n\"Norman Mailer\"\n\"United States\"\n280\n4.7\nnull\nnull\nnull\n4586481\n\"1970 film by Norman Mailer\"\n\"Maidstone is a 1970 American i…\nnull\nnull\n\n\n\"tt2006181\"\n\"Jellyfish Eyes\"\n2013\n\"Japanese\"\n\"Comedy|Fantasy\"\n\"Takashi Murakami\"\n\"Takashi Murakami|Yoshihiro Nis…\n\"Japan\"\n574\n4.7\n29\n34\n101\n41633681\n\"2013 Japanese film\"\n\"Jellyfish Eyes  is a 2013 Japa…\nnull\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt0048434\"\n\"Night and Fog\"\n1956\n\"French\"\n\"Documentary|Short|History\"\n\"Alain Resnais\"\n\"Jean Cayrol\"\n\"France\"\n22148\n8.6\nnull\nnull\n32\n20181860\n\"1956 French film\"\n\"Night and Fog is a 1956 French…\nnull\nnull\n\n\n\"tt0056058\"\n\"Harakiri\"\n1962\n\"Japanese\"\n\"Drama|Mystery\"\n\"Masaki Kobayashi\"\n\"Yasuhiko Takiguchi|Shinobu Has…\n\"Japan\"\n87601\n8.6\n100\n85\n135\n3137571\n\"Japanese jidaigeki film\"\n\"Harakiri  is a 1962 Japanese j…\nnull\n15000\n\n\n\"tt0102926\"\n\"The Silence of the Lambs\"\n1991\n\"English|Latin\"\n\"Crime|Drama|Horror\"\n\"Jonathan Demme\"\n\"Thomas Harris|Ted Tally\"\n\"United States\"\n1667162\n8.6\n95\n86\n118\n30006\n\"1991 horror film by Jonathan D…\n\"The Silence of the Lambs is a …\n19000000\n130742922\n\n\n\"tt0090015\"\n\"Shoah\"\n1985\n\"German|Hebrew|Polish|Yiddish|F…\n\"Documentary|History|War\"\n\"Claude Lanzmann\"\n\"Claude Lanzmann\"\n\"France\"\n11202\n8.7\n100\n99\n540\n147213\n\"1985 French documentary film b…\n\"Shoah is a 1985 French documen…\nnull\nnull\n\n\n\"tt0050083\"\n\"12 Angry Men\"\n1957\n\"English\"\n\"Crime|Drama\"\n\"Sidney Lumet\"\n\"Reginald Rose\"\n\"United States\"\n957726\n9.0\n100\n97\n95\n92605\n\"American legal drama film by S…\n\"12 Angry Men is a 1957 America…\n340000\nnull\n\n\n\n\n\n\nNow, sort the films by runtime with the longest films at the top.\n\n(\n    film\n    .sort(c.runtime_raw, descending=True)\n)\n\n\nshape: (1_470, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0023261\"\n\"No Blood Relation\"\n1932\n\"None|Japanese\"\n\"Drama\"\n\"Mikio Naruse\"\n\"Shunyô Yanagawa|Kôgo Noda\"\n\"Japan\"\n478\n6.8\nnull\nnull\nnull\n66824155\n\"1932 Japanese film\"\n\"No Blood Relation  is a 1932 J…\nnull\nnull\n\n\n\"tt0024214\"\n\"Apart from You\"\n1933\n\"None|Japanese\"\n\"Drama\"\n\"Mikio Naruse\"\n\"Mikio Naruse\"\n\"Japan\"\n666\n7.1\nnull\nnull\nnull\n37563310\n\"1933 Japanese film\"\n\"Apart From You  is a 1933 Japa…\nnull\nnull\n\n\n\"tt0025338\"\n\"Street Without End\"\n1934\n\"None\"\n\"Drama\"\n\"Mikio Naruse\"\n\"Jitsuzô Ikeda|Komatsu Kitamura\"\n\"Japan\"\n396\n6.9\nnull\nnull\nnull\n66481766\n\"1934 Japanese film\"\n\"Street Without End  is a 1934 …\nnull\nnull\n\n\n\"tt0029002\"\n\"History Is Made at Night\"\n1937\n\"English|Italian|French\"\n\"Comedy|Drama|Romance\"\n\"Frank Borzage\"\n\"Gene Towne|C. Graham Baker|Vin…\n\"United States\"\n2615\n7.3\n100\nnull\nnull\n4685891\n\"1937 film by Frank Borzage\"\n\"History Is Made at Night is a …\nnull\nnull\n\n\n\"tt0031828\"\n\"The Tunnel\"\n1940\n\"English\"\n\"Drama|Music\"\n\"Pen Tennyson\"\n\"Herbert Marshall|Alfredda Bril…\n\"United Kingdom\"\n635\n6.7\nnull\nnull\nnull\n3408170\n\"1940 film by Pen Tennyson\"\n\"The Proud Valley is a 1940 Eal…\nnull\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt0056662\"\n\"Vive le tour\"\n1962\n\"French\"\n\"Documentary|Short|Sport\"\n\"Louis Malle\"\n\"Louis Malle\"\n\"France\"\n999\n7.4\nnull\nnull\n18\n22670309\n\"1962 French film\"\n\"Vive le Tour is a 1962 French …\nnull\nnull\n\n\n\"tt0075122\"\n\"The Colours\"\n1976\n\"Persian\"\n\"Documentary|Short\"\n\"Abbas Kiarostami\"\n\"Abbas Kiarostami\"\n\"Iran\"\n397\n6.2\nnull\nnull\n15\n9308455\n\"1976 Iranian film\"\n\"The Colours is a 1976 Iranian …\nnull\nnull\n\n\n\"tt0063551\"\n\"Saute ma ville\"\n1971\n\"French\"\n\"Short|Comedy|Drama\"\n\"Chantal Akerman\"\n\"N/A\"\n\"Belgium\"\n1283\n6.3\nnull\nnull\n13\n308673\n\"Belgian film director, screenw…\n\"Chantal Anne Akerman was a Bel…\nnull\nnull\n\n\n\"tt0068354\"\n\"La chambre\"\n1972\n\"French\"\n\"Short\"\n\"Chantal Akerman\"\n\"Chantal Akerman\"\n\"Belgium|United States\"\n1021\n5.9\nnull\nnull\n11\nnull\nnull\nnull\nnull\nnull\n\n\n\"tt0066127\"\n\"The Bread and Alley\"\n1970\n\"None\"\n\"Drama|Short\"\n\"Abbas Kiarostami\"\n\"Taghi Kiarostami\"\n\"Iran\"\n1550\n7.0\nnull\nnull\n10\n9307076\n\"1970 film\"\n\"The Bread and Alley is a 1970 …\nnull\nnull\n\n\n\n\n\n\nSelect only the films that are less than 60 minutes long (i.e., short films).\n\n(\n    film\n    .filter(c.runtime_raw &lt; 60)\n)\n\n\nshape: (26, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0021576\"\n\"À Propos de Nice\"\n1930\n\"None\"\n\"Documentary|Short|Comedy\"\n\"Boris Kaufman|Jean Vigo\"\n\"Jean Vigo|Boris Kaufman\"\n\"France\"\n4591\n7.4\nnull\nnull\n25\n11286391\n\"1930 film\"\n\"À propos de Nice is a 1930 sil…\nnull\nnull\n\n\n\"tt0022036\"\n\"Flunky, Work Hard!\"\n1931\n\"None|Japanese\"\n\"Short|Comedy|Drama\"\n\"Mikio Naruse\"\n\"Mikio Naruse\"\n\"Japan\"\n531\n6.7\nnull\nnull\n29\n66536197\n\"1931 Japanese film\"\n\"Flunky, Work Hard!  is a 1931 …\nnull\nnull\n\n\n\"tt0024803\"\n\"Zero for Conduct\"\n1933\n\"French\"\n\"Drama\"\n\"Jean Vigo\"\n\"Jean Vigo\"\n\"France\"\n9513\n7.2\n94\nnull\n44\n38244256\n\"1933 French film\"\n\"Zero for Conduct is a 1933 Fre…\nnull\nnull\n\n\n\"tt0028445\"\n\"A Day in the Country\"\n1936\n\"French\"\n\"Short|Comedy|Drama\"\n\"Jean Renoir\"\n\"Guy de Maupassant|Jean Renoir\"\n\"France\"\n7611\n7.5\n100\nnull\n40\n7134551\n\"1946 French film\"\n\"Partie de campagne is a French…\nnull\nnull\n\n\n\"tt0038182\"\n\"The Men Who Tread on the Tiger…\n1945\n\"Japanese\"\n\"Adventure|Drama|Thriller\"\n\"Akira Kurosawa\"\n\"Nobumitsu Kanze|Akira Kurosawa…\n\"Japan\"\n4721\n6.7\n86\nnull\n59\n1529461\n\"1945 Japanese film by Akira Ku…\n\"The Men Who Tread on the Tiger…\nnull\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt0079704\"\n\"Paul Robeson: Tribute to an Ar…\n1979\n\"English|Mandarin\"\n\"Documentary|Short|Biography\"\n\"Saul J. Turell\"\n\"Saul J. Turell\"\n\"United States\"\n521\n7.5\nnull\nnull\n30\n17656522\n\"1979 film\"\n\"Paul Robeson: Tribute to an Ar…\nnull\nnull\n\n\n\"tt0081204\"\n\"Mur murs\"\n1981\n\"French|English|Italian|Spanish\"\n\"Documentary\"\n\"Agnès Varda\"\n\"Agnès Varda\"\n\"France|United States|West Germ…\n1269\n7.4\nnull\nnull\n58\n48394918\n\"1980 documentary film by Agnès…\n\"Mur Murs is a 1981 documentary…\nnull\nnull\n\n\n\"tt0083727\"\n\"Chambre 666\"\n1982\n\"English|French|German|Italian|…\n\"Documentary\"\n\"Wim Wenders\"\n\"Wim Wenders\"\n\"France|West Germany\"\n1877\n6.6\nnull\nnull\n45\n11105410\n\"1982 French film\"\n\"Room 666 is a 1982 documentary…\nnull\nnull\n\n\n\"tt0085641\"\n\"Fellow Citizen\"\n1983\n\"Persian\"\n\"Documentary\"\n\"Abbas Kiarostami\"\n\"Abbas Kiarostami\"\n\"Iran\"\n259\n6.4\nnull\nnull\n48\n9307896\n\"1983 Iranian film\"\n\"Fellow Citizen is a 1983 Irani…\nnull\nnull\n\n\n\"tt0111460\"\n\"Total Balalaika Show\"\n1994\n\"Finnish|English|French|Russian\"\n\"Documentary|Comedy|Music\"\n\"Aki Kaurismäki\"\n\"Aki Kaurismäki\"\n\"Finland\"\n944\n6.9\nnull\nnull\n57\n6345845\n\"1994 Finnish film\"\n\"Total Balalaika Show is a 1994…\nnull\nnull\n\n\n\n\n\n\nSelect all films directed by Charles Chaplin.\n\n(\n    film\n    .filter(c.director == \"Charles Chaplin\")\n)\n\n\nshape: (8, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"None|English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n\"tt0014624\"\n\"A Woman of Paris: A Drama of F…\n1923\n\"English\"\n\"Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n6548\n6.9\n94\n76\n84\n546663\n\"1923 drama film by Charlie Cha…\n\"A Woman of Paris is a 1923 sil…\n351000\n634000\n\n\n\"tt0018773\"\n\"The Circus\"\n1928\n\"None|English\"\n\"Comedy|Family|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n37901\n8.1\n97\n90\n69\n142780\n\"1928 film by Charlie Chaplin\"\n\"The Circus is a 1928 silent ro…\nnull\nnull\n\n\n\"tt0021749\"\n\"City Lights\"\n1931\n\"None|English\"\n\"Comedy|Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin|Harry Carr|Har…\n\"United States\"\n210306\n8.5\n95\n99\n87\n73401\n\"1931 American silent film\"\n\"City Lights is a 1931 American…\n1500000\n5000000\n\n\n\"tt0027977\"\n\"Modern Times\"\n1936\n\"English\"\n\"Comedy|Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n276967\n8.5\n98\n96\n99\n74962\n\"1936 comedy film by Charlie Ch…\n\"Modern Times is a 1936 America…\n1500000\n1400000\n\n\n\"tt0032553\"\n\"The Great Dictator\"\n1940\n\"English|Esperanto|Latin\"\n\"Comedy|Drama|War\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n252228\n8.4\n92\nnull\n125\n62689\n\"1940 American film by Charlie …\n\"The Great Dictator is a 1940 A…\n2000000\n5000000\n\n\n\"tt0039631\"\n\"Monsieur Verdoux\"\n1947\n\"English|French|Latin\"\n\"Comedy|Crime|Drama\"\n\"Charles Chaplin\"\n\"Charles Chaplin|Orson Welles|M…\n\"United States\"\n19567\n7.8\n97\nnull\n124\n545615\n\"1947 film by Charlie Chaplin\"\n\"Monsieur Verdoux is a 1947 Ame…\n2000000\nnull\n\n\n\"tt0044837\"\n\"Limelight\"\n1952\n\"English\"\n\"Comedy|Drama|Music\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n23456\n8.0\n88\n84\n137\n583273\n\"1952 film by Charlie Chaplin\"\n\"Limelight is a 1952 American c…\n900000\n1000000\n\n\n\n\n\n\nSelect all films directed by either Charles Chaplin or Jean-Luc Godard.\n\n(\n    film\n    .filter(c.director.is_in([\"Charles Chaplin\", \"Jean-Luc Godard\"]))\n)\n\n\nshape: (19, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"None|English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n\"tt0014624\"\n\"A Woman of Paris: A Drama of F…\n1923\n\"English\"\n\"Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n6548\n6.9\n94\n76\n84\n546663\n\"1923 drama film by Charlie Cha…\n\"A Woman of Paris is a 1923 sil…\n351000\n634000\n\n\n\"tt0018773\"\n\"The Circus\"\n1928\n\"None|English\"\n\"Comedy|Family|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n37901\n8.1\n97\n90\n69\n142780\n\"1928 film by Charlie Chaplin\"\n\"The Circus is a 1928 silent ro…\nnull\nnull\n\n\n\"tt0021749\"\n\"City Lights\"\n1931\n\"None|English\"\n\"Comedy|Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin|Harry Carr|Har…\n\"United States\"\n210306\n8.5\n95\n99\n87\n73401\n\"1931 American silent film\"\n\"City Lights is a 1931 American…\n1500000\n5000000\n\n\n\"tt0027977\"\n\"Modern Times\"\n1936\n\"English\"\n\"Comedy|Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n276967\n8.5\n98\n96\n99\n74962\n\"1936 comedy film by Charlie Ch…\n\"Modern Times is a 1936 America…\n1500000\n1400000\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt0060675\"\n\"Masculine Feminine\"\n1966\n\"French|Swedish|English\"\n\"Drama|Romance\"\n\"Jean-Luc Godard\"\n\"Jean-Luc Godard|Guy de Maupass…\n\"France|Sweden\"\n18464\n7.4\n96\n93\n104\n1644429\n\"1966 film by Jean-Luc Godard\"\n\"Masculin féminin: 15 Specific …\nnull\nnull\n\n\n\"tt0060304\"\n\"2 or 3 Things I Know About Her\"\n1967\n\"French|Italian|English\"\n\"Comedy|Drama\"\n\"Jean-Luc Godard\"\n\"Catherine Vimenet|Jean-Luc God…\n\"France\"\n8772\n6.5\n94\nnull\n90\n1608743\n\"1967 film by Jean-Luc Godard\"\n\"Two or Three Things I Know Abo…\nnull\nnull\n\n\n\"tt0062480\"\n\"Weekend\"\n1967\n\"French\"\n\"Adventure|Comedy|Drama\"\n\"Jean-Luc Godard\"\n\"Julio Cortázar|Jean-Luc Godard\"\n\"France|Italy\"\n16133\n6.9\n93\nnull\n103\n2117053\n\"Film by Jean-Luc Godard\"\n\"Weekend is a 1967 postmodern b…\nnull\nnull\n\n\n\"tt0079854\"\n\"Every Man for Himself\"\n1980\n\"French|Italian\"\n\"Drama\"\n\"Jean-Luc Godard\"\n\"Anne-Marie Miéville|Jean-Claud…\n\"France|Switzerland|West German…\n4176\n6.5\n90\nnull\n87\n6327478\n\"1980 French film\"\n\"Every Man for Himself is a 198…\nnull\nnull\n\n\n\"tt0093349\"\n\"King Lear\"\n1987\n\"French|English|Russian|Japanes…\n\"Comedy|Drama|Sci-Fi\"\n\"Jean-Luc Godard\"\n\"Richard Debuisne|Jean-Luc Goda…\n\"United States|Bahamas|France|S…\n1510\n5.4\n67\nnull\n90\n13053198\n\"1987 film directed by Jean-Luc…\n\"King Lear is a 1987 film direc…\nnull\nnull\n\n\n\n\n\n\nFind the highest-rated film from each year. Sort the final result by year so it is easier to read.\n\n(\n    film\n    .sort(c.rating_imdb, descending=True)\n    .group_by(c.year)\n    .head(1)\n    .sort(c.year)\n)\n\n\nshape: (102, 18)\n\n\n\nyear\nimdb_id\ntitle\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\ni64\nstr\nstr\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n1921\n\"tt0012349\"\n\"The Kid\"\n\"None|English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n1922\n\"tt0013257\"\n\"Häxan\"\n\"Swedish|Danish\"\n\"Documentary|Fantasy|Horror\"\n\"Benjamin Christensen\"\n\"Benjamin Christensen\"\n\"Sweden|Denmark\"\n18391\n7.6\n93\nnull\n107\n3644898\n\"Swedish 1922 silent horror ess…\n\"Häxan is a 1922 Swedish-Danish…\n2000000\nnull\n\n\n1923\n\"tt0014429\"\n\"Safety Last!\"\n\"English\"\n\"Action|Comedy|Thriller\"\n\"Fred C. Newmeyer|Sam Taylor\"\n\"Hal Roach|Sam Taylor|Tim Whela…\n\"United States\"\n23503\n8.1\n97\nnull\n73\n76313\n\"1923 American silent romantic …\n\"Safety Last! is a 1923 America…\n121000\nnull\n\n\n1925\n\"tt0015841\"\n\"The Freshman\"\n\"None|English\"\n\"Comedy|Family|Romance\"\n\"Fred C. Newmeyer|Sam Taylor\"\n\"Sam Taylor|Ted Wilde|John Grey\"\n\"United States\"\n6373\n7.5\n95\nnull\n76\n3831825\n\"1925 film\"\n\"The Freshman is a 1925 America…\n301681\nnull\n\n\n1927\n\"tt0018528\"\n\"The Unknown\"\n\"None|English\"\n\"Drama|Horror|Romance\"\n\"Tod Browning\"\n\"Tod Browning|Waldemar Young|Jo…\n\"United States\"\n9487\n7.7\n100\nnull\n63\n3566585\n\"1927 American film\"\n\"The Unknown is a 1927 American…\n217000\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2020\n\"tt11874226\"\n\"David Byrne's American Utopia\"\n\"English\"\n\"Documentary|Music|Musical\"\n\"Spike Lee\"\n\"David Byrne\"\n\"United States\"\n5419\n8.2\n97\n93\n105\n64612564\n\"2020 film by Spike Lee\"\n\"American Utopia is a 2020 Amer…\nnull\nnull\n\n\n2021\n\"tt10370710\"\n\"The Worst Person in the World\"\n\"Norwegian\"\n\"Comedy|Drama|Romance\"\n\"Joachim Trier\"\n\"Eskil Vogt|Joachim Trier\"\n\"Norway|France|Sweden|Denmark\"\n113923\n7.7\n96\n91\n121\n67841752\n\"2021 Norwegian romantic dramed…\n\"The Worst Person in the World …\n5600000\n3034775\n\n\n2022\n\"tt14641542\"\n\"The Eight Mountains\"\n\"Italian|English|Nepali\"\n\"Drama\"\n\"Felix van Groeningen|Charlotte…\n\"Paolo Cognetti|Charlotte Vande…\n\"Italy|Belgium|France|United Ki…\n18235\n7.7\n91\n78\n147\n70850533\n\"2022 Italian drama\"\n\"The Eight Mountains is a 2022 …\nnull\nnull\n\n\n2023\n\"tt28490873\"\n\"Ryuichi Sakamoto: Opus\"\n\"Japanese\"\n\"Documentary|Music\"\n\"Neo Sora\"\n\"N/A\"\n\"Japan\"\n970\n8.0\n100\n91\n103\n76603667\n\"2023 documentary film\"\n\"Ryuichi Sakamoto: Opus, graphi…\nnull\nnull\n\n\n2024\n\"tt28607951\"\n\"Anora\"\n\"English|Russian|Armenian\"\n\"Comedy|Drama|Romance\"\n\"Sean Baker\"\n\"Sean Baker\"\n\"United States\"\n235046\n7.4\n93\n91\n139\n75146745\n\"2024 film by Sean Baker\"\n\"Anora is a 2024 American roman…\nnull\nnull\n\n\n\n\n\n\nFor each director, compute the average IMDB rating of their films and the number of films they have in the collection.\n\n(\n    film\n    .group_by(c.director)\n    .agg(\n        rating_imdb_mean = c.rating_imdb.mean(),\n        count = pl.len()\n    )\n)\n\n\nshape: (650, 3)\n\n\n\ndirector\nrating_imdb_mean\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"Nicholas Ray\"\n7.566667\n3\n\n\n\"Kihachi Okamoto\"\n7.45\n2\n\n\n\"Michael Cimino\"\n6.7\n1\n\n\n\"William Klein\"\n6.5\n3\n\n\n\"Giorgio Stegani\"\n6.1\n1\n\n\n…\n…\n…\n\n\n\"Basil Dearden\"\n7.3\n4\n\n\n\"Henry King\"\n7.7\n1\n\n\n\"Brett Morgen\"\n7.6\n1\n\n\n\"Jonathan Demme\"\n7.133333\n3\n\n\n\"Vittorio De Sica\"\n7.633333\n6\n\n\n\n\n\n\nWe often want to group data by decade rather than individual years. To compute the decade from a year, we can use the expression c.year // 10 * 10. The // operator performs integer division (dividing and dropping the remainder), so for example 1987 // 10 gives 198, and multiplying by 10 gives 1980. Create a new column containing the decade each film was released in.\n\n(\n    film\n    .with_columns(\n        decade =  (c.year // 10 * 10)\n    )\n)\n\n\nshape: (1_470, 19)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\ndecade\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"None|English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n1920\n\n\n\"tt0012364\"\n\"The Phantom Carriage\"\n1921\n\"None|Swedish\"\n\"Drama|Fantasy|Horror\"\n\"Victor Sjöström\"\n\"Selma Lagerlöf|Victor Sjöström\"\n\"Sweden\"\n15311\n8.0\n100\nnull\n106\n7329426\n\"1921 film by Victor Sjöström\"\n\"The Phantom Carriage is a 1921…\nnull\nnull\n1920\n\n\n\"tt0013257\"\n\"Häxan\"\n1922\n\"Swedish|Danish\"\n\"Documentary|Fantasy|Horror\"\n\"Benjamin Christensen\"\n\"Benjamin Christensen\"\n\"Sweden|Denmark\"\n18391\n7.6\n93\nnull\n107\n3644898\n\"Swedish 1922 silent horror ess…\n\"Häxan is a 1922 Swedish-Danish…\n2000000\nnull\n1920\n\n\n\"tt0014429\"\n\"Safety Last!\"\n1923\n\"English\"\n\"Action|Comedy|Thriller\"\n\"Fred C. Newmeyer|Sam Taylor\"\n\"Hal Roach|Sam Taylor|Tim Whela…\n\"United States\"\n23503\n8.1\n97\nnull\n73\n76313\n\"1923 American silent romantic …\n\"Safety Last! is a 1923 America…\n121000\nnull\n1920\n\n\n\"tt0014624\"\n\"A Woman of Paris: A Drama of F…\n1923\n\"English\"\n\"Drama|Romance\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n6548\n6.9\n94\n76\n84\n546663\n\"1923 drama film by Charlie Cha…\n\"A Woman of Paris is a 1923 sil…\n351000\n634000\n1920\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt27958252\"\n\"Caught by the Tides\"\n2024\n\"Mandarin|English\"\n\"Drama\"\n\"Jia Zhang-ke\"\n\"Jiahuan Wan|Jia Zhang-ke\"\n\"China|France|Japan\"\n2097\n6.7\n99\n87\n111\n76585127\n\"2024 film by Jia Zhangke\"\n\"Caught by the Tides is a 2024 …\nnull\nnull\n2020\n\n\n\"tt28607951\"\n\"Anora\"\n2024\n\"English|Russian|Armenian\"\n\"Comedy|Drama|Romance\"\n\"Sean Baker\"\n\"Sean Baker\"\n\"United States\"\n235046\n7.4\n93\n91\n139\n75146745\n\"2024 film by Sean Baker\"\n\"Anora is a 2024 American roman…\nnull\nnull\n2020\n\n\n\"tt28618488\"\n\"Vermiglio\"\n2024\n\"Italian|Latin\"\n\"Drama|History\"\n\"Maura Delpero\"\n\"Maura Delpero\"\n\"Italy|France|Belgium\"\n4838\n6.9\n93\n85\n119\n77421920\n\"2024 film by Maura Delpero\"\n\"Vermiglio , also known as The …\nnull\nnull\n2020\n\n\n\"tt32085997\"\n\"Misericordia\"\n2024\n\"French\"\n\"Comedy|Crime|Drama\"\n\"Alain Guiraudie\"\n\"Alain Guiraudie\"\n\"France|Spain|Portugal\"\n2537\n7.1\n94\n83\n103\n76603314\n\"2024 film by Alain Guiraudie\"\n\"Misericordia is a 2024 black c…\nnull\nnull\n2020\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Malayalam|Hindi|Marathi|Englis…\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia|Himanshu Prajapa…\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n2020\n\n\n\n\n\n\nNow, count how many films in the collection were released in each decade.\n\n(\n    film\n    .with_columns(\n        decade =  (c.year // 10 * 10)\n    )\n    .group_by(c.decade)\n    .agg(\n        count = pl.len()   \n    )\n)\n\n\nshape: (11, 2)\n\n\n\ndecade\ncount\n\n\ni64\nu32\n\n\n\n\n1950\n162\n\n\n1930\n119\n\n\n1970\n232\n\n\n2000\n86\n\n\n1920\n19\n\n\n…\n…\n\n\n2020\n47\n\n\n1980\n171\n\n\n1960\n303\n\n\n2010\n55\n\n\n1990\n154\n\n\n\n\n\n\nFind the directors with more than 5 films in the collection, showing their average IMDB rating. Sort the results so the highest-rated directors appear at the top.\n\n(\n    film\n    .group_by(c.director)\n    .agg(\n        rating_imdb_mean = c.rating_imdb.mean(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 5)\n    .sort(c.rating_imdb_mean, descending=True)\n)\n\n\nshape: (48, 3)\n\n\n\ndirector\nrating_imdb_mean\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"Charles Chaplin\"\n8.05\n8\n\n\n\"Satyajit Ray\"\n7.981818\n11\n\n\n\"Kenji Mizoguchi\"\n7.777778\n9\n\n\n\"Richard Linklater\"\n7.766667\n6\n\n\n\"Carl Theodor Dreyer\"\n7.683333\n6\n\n\n…\n…\n…\n\n\n\"Pier Paolo Pasolini\"\n6.941667\n12\n\n\n\"Nagisa Ôshima\"\n6.85\n8\n\n\n\"Keisuke Kinoshita\"\n6.842857\n7\n\n\n\"David Cronenberg\"\n6.742857\n7\n\n\n\"Chantal Akerman\"\n6.7375\n8\n\n\n\n\n\n\nFor each director with more than 5 films in the collection, show their highest-rated film. Hint: If you sort by rating before grouping, the first title in each group will be that director’s highest-rated film.\n\n(\n    film\n    .sort(c.rating_imdb, descending=True)\n    .group_by(c.director)\n    .agg(\n        title = c.title.first(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 5)\n)\n\n\nshape: (48, 3)\n\n\n\ndirector\ntitle\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"Federico Fellini\"\n\"La Strada\"\n7\n\n\n\"Ingmar Bergman\"\n\"Scenes from a Marriage\"\n39\n\n\n\"John Cassavetes\"\n\"A Woman Under the Influence\"\n7\n\n\n\"Pier Paolo Pasolini\"\n\"Accattone\"\n12\n\n\n\"Koreyoshi Kurahara\"\n\"Intimidation\"\n6\n\n\n…\n…\n…\n\n\n\"Roberto Rossellini\"\n\"Rome, Open City\"\n8\n\n\n\"Michael Haneke\"\n\"The Seventh Continent\"\n6\n\n\n\"Masaki Kobayashi\"\n\"Harakiri\"\n7\n\n\n\"Charles Chaplin\"\n\"City Lights\"\n8\n\n\n\"Satyajit Ray\"\n\"The World of Apu\"\n11\n\n\n\n\n\n\nBy default, only the first few rows of a result are displayed. To see more rows, we can pipe the result through the print_rows function. Modify your previous answer to display all of the results.\n\n(\n    film\n    .sort(c.rating_imdb, descending=True)\n    .group_by(c.director)\n    .agg(\n        title = c.title.first(),\n        count = pl.len()\n    )\n    .filter(c.count &gt; 5)\n    .pipe(print_rows)\n)\n\n\nshape: (48, 3)\n\n\n\ndirector\ntitle\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"Federico Fellini\"\n\"La Strada\"\n7\n\n\n\"David Lynch\"\n\"Mulholland Drive\"\n6\n\n\n\"Alfred Hitchcock\"\n\"Rebecca\"\n7\n\n\n\"John Cassavetes\"\n\"A Woman Under the Influence\"\n7\n\n\n\"Satyajit Ray\"\n\"The World of Apu\"\n11\n\n\n\"Roberto Rossellini\"\n\"Rome, Open City\"\n8\n\n\n\"Martin Scorsese\"\n\"The Last Waltz\"\n8\n\n\n\"Kenji Mizoguchi\"\n\"Sansho the Bailiff\"\n9\n\n\n\"Carl Theodor Dreyer\"\n\"Ordet\"\n6\n\n\n\"Wong Kar-Wai\"\n\"In the Mood for Love\"\n7\n\n\n\"Pier Paolo Pasolini\"\n\"Accattone\"\n12\n\n\n\"Ernst Lubitsch\"\n\"To Be or Not to Be\"\n8\n\n\n\"Louis Malle\"\n\"Au Revoir les Enfants\"\n13\n\n\n\"Seijun Suzuki\"\n\"Youth of the Beast\"\n7\n\n\n\"Richard Linklater\"\n\"Before Sunrise\"\n6\n\n\n\"Ingmar Bergman\"\n\"Scenes from a Marriage\"\n39\n\n\n\"Agnès Varda\"\n\"The Beaches of Agnès\"\n17\n\n\n\"Akira Kurosawa\"\n\"Seven Samurai\"\n24\n\n\n\"David Lean\"\n\"Brief Encounter\"\n7\n\n\n\"Nagisa Ôshima\"\n\"Death by Hanging\"\n8\n\n\n\"Chantal Akerman\"\n\"Jeanne Dielman, 23, quai du Co…\n8\n\n\n\"Koreyoshi Kurahara\"\n\"Intimidation\"\n6\n\n\n\"Yasujirô Ozu\"\n\"Late Spring\"\n20\n\n\n\"Abbas Kiarostami\"\n\"Close-Up\"\n26\n\n\n\"Wes Anderson\"\n\"The Grand Budapest Hotel\"\n10\n\n\n\"Mikio Naruse\"\n\"When a Woman Ascends the Stair…\n6\n\n\n\"Jacques Demy\"\n\"The Umbrellas of Cherbourg\"\n6\n\n\n\"Jim Jarmusch\"\n\"Down by Law\"\n6\n\n\n\"Aki Kaurismäki\"\n\"The Bohemian Life\"\n9\n\n\n\"Masaki Kobayashi\"\n\"Harakiri\"\n7\n\n\n\"Rainer Werner Fassbinder\"\n\"Ali: Fear Eats the Soul\"\n10\n\n\n\"Jean-Luc Godard\"\n\"Vivre sa vie\"\n11\n\n\n\"Charles Chaplin\"\n\"City Lights\"\n8\n\n\n\"Kenji Misumi\"\n\"Lone Wolf and Cub: Baby Cart a…\n10\n\n\n\"Michael Haneke\"\n\"The Seventh Continent\"\n6\n\n\n\"Vittorio De Sica\"\n\"Bicycle Thieves\"\n6\n\n\n\"Wim Wenders\"\n\"Paris, Texas\"\n12\n\n\n\"François Truffaut\"\n\"The 400 Blows\"\n7\n\n\n\"Jean Renoir\"\n\"The Rules of the Game\"\n10\n\n\n\"Terry Gilliam\"\n\"Brazil\"\n6\n\n\n\"David Cronenberg\"\n\"A History of Violence\"\n7\n\n\n\"Robert Bresson\"\n\"A Man Escaped\"\n6\n\n\n\"Michael Powell|Emeric Pressbur…\n\"The Red Shoes\"\n7\n\n\n\"Keisuke Kinoshita\"\n\"Twenty-Four Eyes\"\n7\n\n\n\"Marlon Riggs\"\n\"Ethnic Notions\"\n6\n\n\n\"Julien Duvivier\"\n\"Panique\"\n6\n\n\n\"Michelangelo Antonioni\"\n\"La Notte\"\n7\n\n\n\"Samuel Fuller\"\n\"Pickup on South Street\"\n7",
    "crumbs": [
      "Notebook02c"
    ]
  },
  {
    "objectID": "notebook03a.html",
    "href": "notebook03a.html",
    "title": "Notebook03a",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\").drop(c.description)\n\n\n\nQuestions\nWe will continue working with the food dataset, this time focusing on visualizations using plotnine. Remember to look at your output after each question to understand what each piece of code does.\nCreate a scatter plot with vitamin A on the x-axis and iron on the y-axis.\n\n(\n    food\n    .pipe(ggplot, aes(\"vitamin_a\", \"iron\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nNow create a scatter plot with protein on the x-axis and fiber on the y-axis.\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nLet’s add some data transformation before we plot the data. Recreate the previous plot, but only show the fruits.\n\n(\n    food\n    .filter(c.food_group == \"fruit\")\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nFinally (yes, this is a short notebook), let’s aggregate the data before plotting it. Group by food group and count the number of rows in each group. Create a scatter plot that has the food group on the x-axis and the count on the y-axis.\n\n(\n    food\n    .group_by(c.food_group)\n    .agg(count = pl.len())\n    .pipe(ggplot, aes(\"food_group\", \"count\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nThink about what changes you would like to make to this plot to make it more readable and interesting. We will get to (most) of those next time!",
    "crumbs": [
      "Notebook03a"
    ]
  },
  {
    "objectID": "notebook03c.html",
    "href": "notebook03c.html",
    "title": "Notebook03c",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nmovie = pl.read_csv(ub + \"data/movies_50_years.csv\")\n\n\n\nQuestions\nIn this notebook, we will explore data visualization techniques using a dataset of popular movies from the past 50 years. The movie dataset includes information about each film’s runtime (in minutes), release year, MPA rating (G, PG, PG-13, R), IMDB rating, and the number of ratings it received.\nStart by creating a histogram of movie runtimes.\n\n(\n    movie\n    .pipe(ggplot, aes(\"runtime\"))\n    + geom_histogram(fill=\"white\", color=\"black\")\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/stats/stat_bin.py:112: PlotnineWarning: 'stat_bin()' using 'bins = 158'. Pick better value with 'binwidth'.\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_bin : Removed 216 rows containing non-finite values.\n\n\n\n\n\n\n\n\n\nThe default bin settings make it hard to interpret. Modify your histogram to use a bin width of 10 minutes, with bins starting at 0.\n\n(\n    movie\n    .pipe(ggplot, aes(\"runtime\"))\n    + geom_histogram(fill=\"white\", color=\"black\", binwidth=10, boundary=0)\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_bin : Removed 216 rows containing non-finite values.\n\n\n\n\n\n\n\n\n\nThere are some extremely long films in the dataset. Filter to keep only movies shorter than 3 hours, convert the runtime to hours, and create a histogram with 15-minute bins (that’s 0.25 hours).\n\n(\n    movie\n    .filter(c.runtime &lt; 3 * 60)\n    .with_columns(\n        runtime_hours = c.runtime / 60\n    )\n    .pipe(ggplot, aes(\"runtime_hours\"))\n    + geom_histogram(fill=\"white\", color=\"black\", binwidth=0.25, boundary=0)\n)\n\n\n\n\n\n\n\n\nWithin a 15-minute bucket, what is the most popular duration for a film in this dataset?\nAnswer: It’s 1h30m to 1h45m. A fairly typical time for movies of all genres across the past fifty years.\nCreate a boxplot showing the distribution of runtimes for each MPA rating category.\n\n(\n    movie\n    .pipe(ggplot, aes(\"mpa\", \"runtime\"))\n    + geom_boxplot()\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_boxplot : Removed 453 rows containing non-finite values.\n\n\n\n\n\n\n\n\n\nLet’s see how runtimes have changed over time. Filter to movies under 3 hours, create a column for the 5-year period each movie belongs to, and make a boxplot of runtime by period. Use factor() around the period variable so that it is treated as a categorical variable. Hint: The 5-year period can be done the same way we did for decades, but now with 5 replacing 10 in the integer division.\n\n(\n    movie\n    .filter(c.runtime &lt; 3 * 60)\n    .with_columns(\n        period = c.year // 5 * 5\n    )\n    .pipe(ggplot, aes(\"factor(period)\", \"runtime\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\n\nNow let’s look at how audience engagement has changed. Group the data by 5-year periods and compute the average number of ratings per movie in each period. Create a scatter plot of this trend and add a linear trend line.\n\n(\n    movie\n    .with_columns(\n        period = c.year // 5 * 5\n    )\n    .group_by(c.period)\n    .agg(\n        rating_count_mean = c.rating_count.mean()\n    )\n    .pipe(ggplot, aes(\"period\", \"rating_count_mean\"))\n    + geom_point()\n    + geom_smooth(method=\"lm\", se=False)\n)\n\n\n\n\n\n\n\n\nFilter to just movies from 2010 and create a scatter plot with the number of ratings on the x-axis and the IMDB rating on the y-axis. Color the points by MPA rating using the color-blind friendly color scale.\n\n(\n    movie\n    .filter(c.year == 2010)\n    .pipe(ggplot, aes(\"rating_count\", \"rating\"))\n    + geom_point(aes(color=\"mpa\"))\n    + scale_color_cmap_d()\n)\n\n\n\n\n\n\n\n\nThe rating counts span a huge range, making most points cluster on the left. Modify the previous plot to use a logarithmic scale on the x-axis so we can better see the full distribution.\n\n(\n    movie\n    .filter(c.year == 2010)\n    .pipe(ggplot, aes(\"rating_count\", \"rating\"))\n    + geom_point(aes(color=\"mpa\"))\n    + scale_color_cmap_d()\n    + scale_x_log10()\n)",
    "crumbs": [
      "Notebook03c"
    ]
  },
  {
    "objectID": "notebook03e.html",
    "href": "notebook03e.html",
    "title": "Notebook03e",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfood = pl.read_csv(ub + \"data/food.csv\").drop(c.description)\n\n\n\nQuestions\nWe will continue working with the food dataset, this time focusing on visualizations using plotnine. Remember to look at your output after each question to understand what each piece of code does.\nCreate a scatter plot with vitamin A on the x-axis and iron on the y-axis.\n\n(\n    food\n    .pipe(ggplot, aes(\"vitamin_a\", \"iron\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nNow create a scatter plot with protein on the x-axis and fiber on the y-axis.\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point()\n)\n\n\n\n\n\n\n\n\nRecreate the previous plot, but this time color the points by food group. Notice that color goes inside aes() when mapping to a variable.\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point(aes(color=\"food_group\"))\n)\n\n\n\n\n\n\n\n\nNow, create the same plot but use the color-blind friendly scale scale_color_cmap_d().\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point(aes(color=\"food_group\"))\n    + scale_color_cmap_d()\n)\n\n\n\n\n\n\n\n\nUse a Color Wheel to select a fun color. In the code below, update the previous plot to use this fixed color in place of the food group-based colors. Notice that when using a fixed color (not mapped to data), color goes outside of aes().\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_point(color=\"#E3763B\")\n)\n\n\n\n\n\n\n\n\nInstead of points, use text labels to display the item names. Replace geom_point() with geom_text() and map the label aesthetic to the item column.\n\n(\n    food\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_text(aes(label=\"item\"))\n)\n\n\n\n\n\n\n\n\nThat’s hard to read with so many items! We should filter the data to only include fruits, then create the same text plot. To do this, add .filter(c.food_group == \"fruit\") as the first line of the chain before calling .pipe.\n\n(\n    food\n    .filter(c.food_group == \"fruit\")\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_text(aes(label=\"item\"))\n)\n\n\n\n\n\n\n\n\nFinally, add both points and text labels to the plot. Use nudge_y to shift the text up slightly so it doesn’t overlap with the points.\n\n(\n    food\n    .filter(c.food_group == \"fruit\")\n    .pipe(ggplot, aes(\"protein\", \"fiber\"))\n    + geom_text(aes(label=\"item\"), nudge_y=0.25)\n    + geom_point()\n)",
    "crumbs": [
      "Notebook03e"
    ]
  },
  {
    "objectID": "notebook04b.html",
    "href": "notebook04b.html",
    "title": "Notebook04b",
    "section": "",
    "text": "Setup\nRun all of the following before starting the notebook.\n\n! wget -q -nc https://raw.githubusercontent.com/taylor-arnold/fds-py/refs/heads/main/funs.py\n\n\nimport numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"\n\n\nfilm = pl.read_csv(ub + \"data/criterion.csv\")\n\n\n\nQuestions\nThe Criterion Films dataset effecively has multiple tables inside of it. However, these are stored inside a single larger table keyed on the movie. The language, genre, director, writer, and country are stored with one or more values pushed together with a vertical pipe. Using the text from the notes, we can convert this into data with a unit of observation about any of these columns. And, what’s nice about this is we get a join with the movie-data for free: all of those columns already exist.\nTo get a sense of what this looks like, create a version of the data with one row for each film and language pair. You should see that the number of rows increases from the original dataset.\n\n(\n    film\n    .with_columns(\n        language = c.language.str.split(\"|\")\n    )\n    .explode(c.language)\n)\n\n\nshape: (2_605, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"None\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n\"tt0012364\"\n\"The Phantom Carriage\"\n1921\n\"None\"\n\"Drama|Fantasy|Horror\"\n\"Victor Sjöström\"\n\"Selma Lagerlöf|Victor Sjöström\"\n\"Sweden\"\n15311\n8.0\n100\nnull\n106\n7329426\n\"1921 film by Victor Sjöström\"\n\"The Phantom Carriage is a 1921…\nnull\nnull\n\n\n\"tt0012364\"\n\"The Phantom Carriage\"\n1921\n\"Swedish\"\n\"Drama|Fantasy|Horror\"\n\"Victor Sjöström\"\n\"Selma Lagerlöf|Victor Sjöström\"\n\"Sweden\"\n15311\n8.0\n100\nnull\n106\n7329426\n\"1921 film by Victor Sjöström\"\n\"The Phantom Carriage is a 1921…\nnull\nnull\n\n\n\"tt0013257\"\n\"Häxan\"\n1922\n\"Swedish\"\n\"Documentary|Fantasy|Horror\"\n\"Benjamin Christensen\"\n\"Benjamin Christensen\"\n\"Sweden|Denmark\"\n18391\n7.6\n93\nnull\n107\n3644898\n\"Swedish 1922 silent horror ess…\n\"Häxan is a 1922 Swedish-Danish…\n2000000\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Hindi\"\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia|Himanshu Prajapa…\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Marathi\"\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia|Himanshu Prajapa…\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"English\"\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia|Himanshu Prajapa…\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"German\"\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia|Himanshu Prajapa…\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32163934\"\n\"Flow\"\n2024\n\"Norwegian\"\n\"Documentary|Short\"\n\"Anne Haugsgjerd\"\n\"Anne Haugsgjerd\"\n\"Norway\"\nnull\nnull\n97\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\n\n\n\nUsing the exploded language data, count the number of films in each language and create a histogram of these counts for languages with counts less than 50. Use a bin width of 5.\n\n(\n    film\n    .with_columns(\n        language = c.language.str.split(\"|\")\n    )\n    .explode(c.language)\n    .group_by(c.language)\n    .agg(count = pl.len())\n    .filter(c.count &lt; 50)\n    .pipe(ggplot, aes(\"count\"))\n    + geom_histogram(fill=\"white\", color=\"black\", binwidth=5, boundary=0)\n)\n\n\n\n\n\n\n\n\nUsing the exploded language data, compute the average IMDB rating and the number of films for each language. Create a scatter plot with the count on the x-axis and the average rating on the y-axis.\n\n(\n    film\n    .with_columns(\n        language = c.language.str.split(\"|\")\n    )\n    .explode(c.language)\n    .group_by(c.language)\n    .agg(\n        rating_mean = c.rating_imdb.mean(),\n        count = pl.len()\n    )\n    .pipe(ggplot, aes(\"count\", \"rating_mean\"))\n    + geom_point()\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:374: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\n\n\n\n\n\nFilter to only these languages: English, French, Japanese, Italian, and German. Create a boxplot showing the distribution of runtimes for films in each of these languages.\n\n(\n    film\n    .with_columns(\n        language = c.language.str.split(\"|\")\n    )\n    .explode(c.language)\n    .filter(c.language.is_in([\"English\", \"French\", \"Japanese\", \"Italian\", \"German\"]))\n    .pipe(ggplot, aes(\"language\", \"runtime_raw\"))\n    + geom_boxplot()\n)\n\n/Users/admin/gh/fds-py-nb/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_boxplot : Removed 55 rows containing non-finite values.\n\n\n\n\n\n\n\n\n\nNow, we are going to compute the number of films from each country, sorting from the most to the least and taking the top 10 countries. We will count a country anytime it is listed as at least one of the countries a film was produced in. Save the dataset as top_countries and then make sure to still print it out on the last line. Do any of the responses surprise you?\n\ntop_countries = (\n    film\n    .with_columns(\n        country = c.country.str.split(\"|\")\n    )\n    .explode(c.country)\n    .group_by(c.country)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .head(10)\n)\n\nNow, count the combinations of decade and country, keeping only those countries that are in the top_countries dataset. Use a semi-join for the filtering and sort the final results by count, but keep all of the rows. You do not need to save the output.\n\n(\n    film\n    .with_columns(\n        country = c.country.str.split(\"|\"),\n        decade = c.year // 10 * 10\n    )\n    .explode(c.country)\n    .join(top_countries, on=c.country)\n    .group_by(c.country, c.decade)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n)\n\n\nshape: (85, 3)\n\n\n\ncountry\ndecade\ncount\n\n\nstr\ni64\nu32\n\n\n\n\n\"United States\"\n1970\n85\n\n\n\"United States\"\n1980\n85\n\n\n\"United States\"\n1990\n84\n\n\n\"Japan\"\n1960\n80\n\n\n\"France\"\n1960\n73\n\n\n…\n…\n…\n\n\n\"France\"\n1920\n1\n\n\n\"Italy\"\n1990\n1\n\n\n\"Germany\"\n1920\n1\n\n\n\"Germany\"\n1940\n1\n\n\n\"Sweden\"\n2010\n1\n\n\n\n\n\n\nNow, add to what you have above a pivot that puts the countries in the columns of the dataset. Use the method .fill_null(0) afterwards to fill in the missing values (these are combinations that never occur). The final table is a way of seeing how the number of films from each country in the collection changes by decade.\n\n(\n    film\n    .with_columns(\n        country = c.country.str.split(\"|\"),\n        decade = c.year // 10 * 10\n    )\n    .explode(c.country)\n    .join(top_countries, on=c.country)\n    .group_by(c.country, c.decade)\n    .agg(count = pl.len())\n    .sort(c.count, descending=True)\n    .pivot(index=\"decade\", on=\"country\", values=\"count\")\n    .fill_null(0)\n    .sort(c.decade)\n    .pipe(print_rows)\n)\n\n\nshape: (11, 11)\n\n\n\ndecade\nUnited States\nJapan\nFrance\nItaly\nWest Germany\nUnited Kingdom\nSweden\nGermany\nHong Kong\nCanada\n\n\ni64\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n1920\n13\n0\n1\n0\n0\n1\n2\n1\n0\n0\n\n\n1930\n40\n19\n31\n0\n0\n15\n5\n7\n0\n0\n\n\n1940\n50\n17\n16\n8\n0\n25\n6\n1\n0\n0\n\n\n1950\n47\n35\n31\n24\n2\n13\n13\n0\n0\n0\n\n\n1960\n55\n80\n73\n48\n9\n21\n22\n0\n2\n3\n\n\n1970\n85\n22\n49\n21\n29\n17\n9\n0\n10\n5\n\n\n1980\n85\n9\n30\n6\n16\n25\n4\n0\n9\n4\n\n\n1990\n84\n11\n40\n1\n0\n26\n3\n10\n13\n7\n\n\n2000\n42\n4\n23\n6\n0\n10\n3\n14\n6\n6\n\n\n2010\n32\n3\n21\n6\n0\n8\n1\n9\n0\n0\n\n\n2020\n22\n5\n23\n4\n0\n9\n6\n9\n0\n4\n\n\n\n\n\n\nLet’s do one more analysis with two columns split apart. This creates a dataset with all combinations of the two categories. Note that we have to do two different calls to .explode. Start by expanding the director and writer columns.\n\n(\n    film\n    .with_columns(\n        writer = c.writer.str.split(\"|\"),\n        director = c.director.str.split(\"|\")\n    )\n    .explode(c.writer)\n    .explode(c.director)\n)\n\n\nshape: (3_212, 18)\n\n\n\nimdb_id\ntitle\nyear\nlanguage\ngenre\ndirector\nwriter\ncountry\nimdb_votes\nrating_imdb\nrating_rt\nrating_mc\nruntime_raw\nwikipedia_pageid\nwikipedia_description\nwikipedia_extract\nbudget_raw\nbox_office_raw\n\n\nstr\nstr\ni64\nstr\nstr\nstr\nstr\nstr\ni64\nf64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n\"tt0012349\"\n\"The Kid\"\n1921\n\"None|English\"\n\"Comedy|Drama|Family\"\n\"Charles Chaplin\"\n\"Charles Chaplin\"\n\"United States\"\n142797\n8.2\n100\nnull\n68\n1346905\n\"1921 silent film by Charlie Ch…\n\"The Kid is a 1921 American sil…\n250000\nnull\n\n\n\"tt0012364\"\n\"The Phantom Carriage\"\n1921\n\"None|Swedish\"\n\"Drama|Fantasy|Horror\"\n\"Victor Sjöström\"\n\"Selma Lagerlöf\"\n\"Sweden\"\n15311\n8.0\n100\nnull\n106\n7329426\n\"1921 film by Victor Sjöström\"\n\"The Phantom Carriage is a 1921…\nnull\nnull\n\n\n\"tt0012364\"\n\"The Phantom Carriage\"\n1921\n\"None|Swedish\"\n\"Drama|Fantasy|Horror\"\n\"Victor Sjöström\"\n\"Victor Sjöström\"\n\"Sweden\"\n15311\n8.0\n100\nnull\n106\n7329426\n\"1921 film by Victor Sjöström\"\n\"The Phantom Carriage is a 1921…\nnull\nnull\n\n\n\"tt0013257\"\n\"Häxan\"\n1922\n\"Swedish|Danish\"\n\"Documentary|Fantasy|Horror\"\n\"Benjamin Christensen\"\n\"Benjamin Christensen\"\n\"Sweden|Denmark\"\n18391\n7.6\n93\nnull\n107\n3644898\n\"Swedish 1922 silent horror ess…\n\"Häxan is a 1922 Swedish-Danish…\n2000000\nnull\n\n\n\"tt0014429\"\n\"Safety Last!\"\n1923\n\"English\"\n\"Action|Comedy|Thriller\"\n\"Fred C. Newmeyer\"\n\"Hal Roach\"\n\"United States\"\n23503\n8.1\n97\nnull\n73\n76313\n\"1923 American silent romantic …\n\"Safety Last! is a 1923 America…\n121000\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"tt32085997\"\n\"Misericordia\"\n2024\n\"French\"\n\"Comedy|Crime|Drama\"\n\"Alain Guiraudie\"\n\"Alain Guiraudie\"\n\"France|Spain|Portugal\"\n2537\n7.1\n94\n83\n103\n76603314\n\"2024 film by Alain Guiraudie\"\n\"Misericordia is a 2024 black c…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Malayalam|Hindi|Marathi|Englis…\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Payal Kapadia\"\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Malayalam|Hindi|Marathi|Englis…\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Himanshu Prajapati\"\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32086077\"\n\"All We Imagine as Light\"\n2024\n\"Malayalam|Hindi|Marathi|Englis…\n\"Drama|Romance\"\n\"Payal Kapadia\"\n\"Naseem Azad\"\n\"France|India|Netherlands|Luxem…\n10253\n7.1\n100\n93\n118\n76585159\n\"2024 film by Payal Kapadia\"\n\"All We Imagine as Light is a 2…\nnull\nnull\n\n\n\"tt32163934\"\n\"Flow\"\n2024\n\"Norwegian\"\n\"Documentary|Short\"\n\"Anne Haugsgjerd\"\n\"Anne Haugsgjerd\"\n\"Norway\"\nnull\nnull\n97\nnull\nnull\nnull\nnull\nnull\nnull\nnull\n\n\n\n\n\n\nNow, determine which films have at least one directory equal to at least one of its writers. Then, compute the average of films over each decade and sort by decade. Be careful about the unit of observation here!\n\n(\n    film\n    .with_columns(\n        writer = c.writer.str.split(\"|\"),\n        director = c.director.str.split(\"|\"),\n        decade = c.year // 10 * 10\n    )\n    .explode(c.writer)\n    .explode(c.director)\n    .group_by(c.title, c.decade)\n    .agg(\n        any_match = (c.writer == c.director).any()\n    )\n    .group_by(c.decade)\n    .agg(\n        proportion_match = c.any_match.mean()\n    )\n    .sort(c.decade)\n    .pipe(print_rows)\n)\n\n\nshape: (11, 2)\n\n\n\ndecade\nproportion_match\n\n\ni64\nf64\n\n\n\n\n1920\n0.736842\n\n\n1930\n0.436975\n\n\n1940\n0.467213\n\n\n1950\n0.617284\n\n\n1960\n0.636964\n\n\n1970\n0.698276\n\n\n1980\n0.719298\n\n\n1990\n0.75817\n\n\n2000\n0.825581\n\n\n2010\n0.813559\n\n\n2020\n0.807692\n\n\n\n\n\n\nFinish by turning the plot into a visualization with decade on the x-axis, proportion of the y-axis, using a line geometry. Make the x-axis have breaks every decade and the y-axis limits go from 0 to 1 to make the scale of the plot easier to comprehend.\n\n(\n    film\n    .with_columns(\n        writer = c.writer.str.split(\"|\"),\n        director = c.director.str.split(\"|\"),\n        decade = c.year // 10 * 10\n    )\n    .explode(c.writer)\n    .explode(c.director)\n    .group_by(c.title, c.decade)\n    .agg(\n        any_match = (c.writer == c.director).any()\n    )\n    .group_by(c.decade)\n    .agg(\n        proportion_match = c.any_match.mean()\n    )\n    .sort(c.decade)\n    .pipe(ggplot, aes(\"decade\", \"proportion_match\"))\n    + geom_line()\n    + scale_x_continuous(breaks=breaks_width(10))\n    + scale_y_continuous(limits=[0, 1])\n)",
    "crumbs": [
      "Notebook04b"
    ]
  },
  {
    "objectID": "notebook05a.html",
    "href": "notebook05a.html",
    "title": "Notebook05a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook05a"
    ]
  },
  {
    "objectID": "notebook06a.html",
    "href": "notebook06a.html",
    "title": "Notebook06a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook06a"
    ]
  },
  {
    "objectID": "notebook07a.html",
    "href": "notebook07a.html",
    "title": "Notebook07a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook07a"
    ]
  },
  {
    "objectID": "notebook08a.html",
    "href": "notebook08a.html",
    "title": "Notebook08a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook08a"
    ]
  },
  {
    "objectID": "notebook09a.html",
    "href": "notebook09a.html",
    "title": "Notebook09a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook09a"
    ]
  },
  {
    "objectID": "notebook10a.html",
    "href": "notebook10a.html",
    "title": "Notebook10a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook10a"
    ]
  },
  {
    "objectID": "notebook11a.html",
    "href": "notebook11a.html",
    "title": "Notebook11a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook11a"
    ]
  },
  {
    "objectID": "notebook12a.html",
    "href": "notebook12a.html",
    "title": "Notebook12a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook12a"
    ]
  },
  {
    "objectID": "notebook13a.html",
    "href": "notebook13a.html",
    "title": "Notebook13a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook13a"
    ]
  },
  {
    "objectID": "notebook14a.html",
    "href": "notebook14a.html",
    "title": "Notebook14a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook14a"
    ]
  },
  {
    "objectID": "notebook15a.html",
    "href": "notebook15a.html",
    "title": "Notebook15a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook15a"
    ]
  },
  {
    "objectID": "notebook16a.html",
    "href": "notebook16a.html",
    "title": "Notebook16a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook16a"
    ]
  },
  {
    "objectID": "notebook17a.html",
    "href": "notebook17a.html",
    "title": "Notebook17a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook17a"
    ]
  },
  {
    "objectID": "notebook18a.html",
    "href": "notebook18a.html",
    "title": "Notebook18a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook18a"
    ]
  },
  {
    "objectID": "notebook19a.html",
    "href": "notebook19a.html",
    "title": "Notebook19a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook19a"
    ]
  },
  {
    "objectID": "notebook20a.html",
    "href": "notebook20a.html",
    "title": "Notebook20a",
    "section": "",
    "text": "import numpy as np\nimport polars as pl\n\nfrom funs import *\nfrom plotnine import *\nfrom polars import col as c\ntheme_set(theme_minimal())\n\nub = \"https://raw.githubusercontent.com/taylor-arnold/fds-py-nb/refs/heads/main/\"",
    "crumbs": [
      "Notebook20a"
    ]
  }
]